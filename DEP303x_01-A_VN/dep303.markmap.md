# Big Data with Spark

## Lab list

- [Lab 1]
- [Lab 2]
- [Lab 3]
- [Lab 4]
- [Lab 5]
- [Lab 6]
- [Lab 7]
- [Lab 8]
- [Lab 9]
- [Lab 10]
- [Assessment 1]
- [Lab 11]
- [Lab 12]
- [Lab 13]
- [Assessment 2]

## Big Data and Hadoop

### Big Data

#### Big Data concept

- Big Data là thuật ngữ
  dùng để chỉ 1 tập hợp dữ liệu rất lớn và phức tạp
  đến nỗi những công cụ, ứng dụng xử lý dữ liệu truyền thống
  không thể thu thập, quản lý và xử lý dữ liệu trong một khoản thời gian hợp lý
- Những tập hợp dữ liệu lớn này có thể bao gồm
  - các dữ liệu có cấu trúc **(structured data)**
  - các dữ liệu không cấu trúc **(unstructured data)** như video, file doc,...
  - các dữ liệu bán cấu trúc **(semi structured data)**
- Mỗi tập hợp có chút khác biệt
- Vòng đời của Big Data sẽ diễn ra như sau
  - **Business Case**
    Ở bước này, chúng ta sẽ quyết định các định dạng dữ liệu nào được thu thập trên các yêu cầu nghiệp vụ
  - **Data Collection**
    Lúc này dữ liệu sẽ được thu thập và lưu trữ phân tán thông qua HDFS
  - **Data Modelling**
    Để đảm bảo dữ liệu được lưu trữ đầy đủ, ta cần tạo ra các data model để lưu trữ và xác định mối quan hệ giữa các dữ liệu với nhau (Map Reduce và YARN)
  - **Data Processing**
    Sau khi được mô hình hóa, dữ liệu sẽ cần được xử lý để chỉ lấy các thông tin cần thiết (Spark, Pig, Hive, ...)
  - **Data Visualization**
    Dữ liệu cần được trực quan hóa thành các biểu đồ để người dùng có thể sử dụng dữ liệu cho các vấn đề nghiệp vụ
  - ![Big data life cycle](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_1..png?alt=media&token=b23caf48-e2b9-4e0e-a912-ca9ad5d1f4ff)
- Ngoài ra, Big Data cũng có các **đặc trưng (5V)** như sau:
  - **Volume (khối lượng)**
    Khối lượng của dữ liệu Big Data đang tăng lên mạnh mẽ từng ngày
  - **Velocity (tốc độ)**
    Với sự ra đời của các kỹ thuật, công cụ, ứng dụng lưu trữ
    nguồn dữ liệu lớn liên tục được bổ sung với tốc độ nhanh chóng
  - **Veriety (đa dạng)**
    Sự đa dạng của dữ liệu đến từ nhiều nguồn khác nhau
    như từ các thiết bị cảm biến, thiết bị di động, hay thông qua các trang mạng xã hội
  - **Veracity (tin cậy)**
    Bằng những phương tiện truyền thông xã hội bùng nổ như hiện nay
    và sự gia tăng mạnh mẽ về tương tác và chia sẻ của người dùng
    bức tranh toàn cảnh về độ chính xác cũng như tin cậy của thông tin
    ngày càng trở nên hỗn loạn và khó khăn
    Vấn đề phân tích và loại bỏ dữ liệu thiếu chính xác
    đang là một trong những vấn đề lớn của Big Data
  - **Value (giá trị)**
    Đây là đặc trưng **quan trọng nhất** bởi các thông tin đang tiềm ẩn
    trong bộ dữ liệu khổng lồ được trích xuất
    để phục vụ cho việc phân tích, dự báo
    Những thông tin này có thể được sử dụng trong rất nhiều lĩnh vực
    như kinh doanh, nghiên cứu khoa học, y học, vậy lý,...
    giúp giải quyết những vấn đề mà cuộc sống hiện đại đặt ra
- Videos:
  - [Big Data là gì?](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/24c4V/what-is-big-data)
  - [Tổng quan về Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/yqVpL/beyond-the-hype)
  - [Ảnh hưởng của Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/wLFkJ/impact-of-big-data)
  - [Nên sử dụng Big Data trong trường hợp nào?](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/570eD/big-data-use-cases)
- Ngoài ra, do dữ liệu của chúng ta sẽ được đến từ nhiều nguồn
  và đồng thời cũng sẽ ở nhiều dạng khác nhau
  Vậy nên chúng ta sẽ cần kết hợp các dữ liệu đó lại
  để đồng nhất với nhau và giúp dữ liệu giá trị hơn và liên kết hơn
  Công việc này sẽ bao gồm:
  - Làm giảm độ phức tạp của dữ liệu
  - Tăng mức độ phù hợp của dữ liệu
  - Chuyển đổi để các dữ liệu đồng nhất với nhau
  - Video: [Tích hợp dữ liệu ở các dạng khác nhau](https://www.coursera.org/learn/big-data-introduction/lecture/Ejk8J/the-key-integrating-diverse-data)

#### Ecosystems and tools for Big Data

- Chúng ta có hai cách để có thể xử lý các dữ liệu:
  - **Xử lý tuần tự (Linear processing):**
    - Các công việc xử lý dữ liệu sẽ được thực hiện tuần tự.
    - Các công việc trước cần phải hoàn thành thì mới có thể sang công việc tiếp theo
    - Nếu có lỗi xảy ra ở một bước đó thì các công việc sẽ phải
      dừng lại, hoặc chạy lại hết từ đầu sau khi xử lý các lỗi
  - **Xử lý song song (Parallel processing):**
    - Các công việc được xử lý song song, đọc lập với nhau
    - Nếu có 1 công việc bị lỗi thì cũng sẽ không ảnh hưởng
      đến các công việc còn lại và có thể dễ dàng thực hiện lại
  - ![linear & parallel processing](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_2..png?alt=media&token=0fdc5f0e-46e1-4adb-8637-717184fe9665)
- Có thể thấy rằng việc xử lý dữ liệu song song
  sẽ phù hợp hơn với Big Data nhờ vào một số ưu điểm sau:
  - Tiết kiệm thời gian xử lý dữ liệu
  - Tiết kiệm được các tài nguyên tính toán hơn cho các Node
  - Dễ dàng mở rộng hơn nếu dữ liệu nhiều hơn
- Với Big Data, để xử lý dữ liệu song song thì
  chúng ta sẽ chia hệ thống thành nhiều máy **(Cluster)** khác nhau
  mỗi máy có thể đảm nhiệm vai trò lưu trữ, tính toán hoặc cả hai
  Sau đó, chúng ta cần xử lý dữ liệu thì sẽ chạy song song với các Cluster khác nhau
  Kiến trúc này giúp cho dữ liệu có thể được xử lý song song và độc lập với nhau
- ![parallel processing clusters](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_3..png?alt=media&token=1fcea8af-5bd7-48c2-8db1-6b1da86e7fa6)
- Ngoài ra, kiến trúc này cũng giúp dễ dàng mở rộng theo chiều ngang
  tức là khi ta cần một tài nguyên lớn hơn để tính toán thì chỉ việc tạo thêm nhiều Cluster
  chứ không cần phải nâng cấp cấu hình cho các Cluster cũ
  Đồng thời cũng giúp tính bảo toàn tính toàn vẹn dữ liệu thông qua cơ chế **Fault tolerance**
  dữ liệu sẽ được tạo thành nhiều bản sao ở các Cluster khác nhau
  khi 1 Cluster gặp sự cố gây ra mất mát dữ liệu thì ta có thể dễ dàng backup lại
  Ta sẽ được tìm hiểu kỹ hơn ở bài về Hadoop HDFS
- [parallel processing clusters problem](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_4..png?alt=media&token=544bc364-9996-4696-ae90-fd241d154c2e)
- Video: [Xử lý dữ liệu song song và Scaling](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/ix1eD/parallel-processing-scaling-and-data-parallelism)

- Các công cụ trong hệ sinh thái Big Data sẽ được chia thành các công cụ như sau:
  - **Data Technologies**
    Sử dụng để xử lý, chia sẻ các dữ liệu ở định dạng khác nhau và giúp dữ liệu phân tán
  - **Analytics và Visualization**
    Giúp trực quan hóa và phân tích dữ liệu
  - **Business Intelligence**
    Giúp chuyển hóa các dữ liệu để dễ dàng truy cập, phục vụ cho các vấn đề nghiệp vụ
  - **Cloud Provider**
    Cung cấp các dịch vụ dưới dạng Cloud để dễ dàng mở rộng hơn
  - **NoSQL Database**
    Cơ sở dữ liệu NoSQL để lưu trữ dữ liệu
  - **Programming Tools**
    Các công cụ để xử lý các công việc đòi hỏi logic phức tạp trong vòng đời của Big Data
- Và đồng thời sẽ có những công cụ mã nguồn mở giúp chúng ta thao tác với Big Data dễ dàng hơn
- Video: [Hệ sinh thái và các công cụ về Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/WyhaU/big-data-tools-and-ecosystem)
- Video: [Các công cụ mã nguồn mở của Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/Fk5f4/open-source-and-big-data)

### Hadoop

#### Hadoop concepts

- Hadoop là một dạng Apache Framework
- Apache Hadoop là một mã nguồn mở cho phép sử dụng các distributed processing (ứng dụng phân tán) để quản lý và lưu trữ những tệp dữ liệu lớn
- Hadoop áp dụng mô hình MapReduce trong hoạt động xử lý Big Data
- Hadoop có những ưu điểm sau:
  - Cho phép ng dùng nhanh chóng viết và kiểm tra các hệ thống phân tán
    Đây là cách hiệu quả cho phép phân phối dữ liệu và công việc xuyên suốt các Cluster nhờ vào cơ chế xử lý song song của các lỗi CPU
  - Hệ thống Hadoop thiết kế sao cho các lỗi xảy ra trong hệ thống
    được xử lý tự động, không ảnh hưởng đến các ứng dụng phía trên
  - Có thể phát triển lên nhiều server với cấu trúc master-slave
    để đảm bảo thực các công việc linh hoạt và không bị ngắt quãng
    do chia nhỏ công việc cho các server slave được điều khiển bởi server master
  - Có thể tương thích trên mọi nền tảng như Window, Linux, MacOS do được tạo từ Java
- Tuy nhiên Hadoop vẫn có những khuyết điểm như:
  - thiết bị lưu trữ tốc độ chậm
  - máy tính thiếu tin cậy
  - lập trình song song phân tán không dễ dàng
- Video: [Giới thiệu về Hadoop](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/1sOjx/introduction-to-hadoop)
- Hadoop gồm 4 thành phần chính:
  - **Hadoop Common**
    Đây là các thư viện và tiện ích cần thiết của Java để các module khác sử dụng.
    Những thư viện này cung cấp hệ thống file và lớp OS trừu tượng,
    đồng thời chứa các mã lệnh Java để khởi động Hadoop.
  - **Hadoop YARN**
    Đây là framework để quản lý tiến trình và tài nguyên của các cluster.
  - **Hadoop Distributed File System (HDFS)**
    Đây là hệ thống file phân tán cung cấp truy cập thông lượng cao
    cho ứng dụng khai thác dữ liệu.
  - **Hadoop MapReduce**
    Đây là hệ thống dựa trên YARN dùng để xử lý song song các tập dữ liệu lớn.
- Hiện nay Hadoop đang ngày càng được mở rộng cũng như
  được nhiều framework khác hỗ trợ như Hive, Hbase, Pig.
  Tùy vào mục đích sử dụng mà ta sẽ áp dụng framework phù hợp
  để nâng cao hiệu quả xử lý dữ liệu của Hadoop.

- Video: [Hệ sinh thái Hadoop](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/g7y7n/hadoop-ecosystem)

#### HDFS

- Một trong những vấn đề lớn nhất của các hệ thống phân tích Big Data là quá tải
  Không phải hệ thống nào cũng đủ khỏe để có thể tiếp nhận một lượng thông tin khổng lồ như vậy
  Chính vì thế, nhiệm vụ của **Hadoop Distributed File System** là phân tán cung cấp truy cập thông lượng cao giúp cho ứng dụng chủ
  Cụ thể, khi HDFS nhận được một tệp tin, nó sẽ tự động chia file đó ra thành nhiều phần nhỏ
  Các mảnh nhỏ này được nhân lên nhiều lần và chia ra lưu trữ tại các máy chủ khác nhau
  để phân tán sức nặng mà dữ liệu tạo nên đồng thời cũng đảm bảo được tính toàn vẹn dữ liệu
- HDFS sử dụng các cấu trúc **master node** và **worker/slave node**
  Trong khi master node hay còn gọi là **Name Node** quản lý các file metadata
  thì **worker/slave node** chịu trách nhiệm lưu trữ dữ liệu
  Chính vì thế nên worker/slave node cũng được gọi là data node
  Một **Data node** sẽ chứa nhiều khối được phân nhỏ của tệp tin lớn ban đầu
  Dựa theo chỉ thị từ Master node, các Data node này sẽ trực tiếp điều hành hoạt động thêm, bớt những khối nhỏ của tệp tin
- ![HDFS architecture](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L2_1..png?alt=media&token=137107da-2f66-4c9d-9a2f-944266381b58)
- HDFS chỉ cung cấp 2 cơ chế truy cập dữ liệu là READ và WRITE
  - **READ**
    - Client sẽ gửi yêu cầu đến Name Node để biết được dữ liệu đang nằm ở Data Node nào.
      Sau đó sẽ trực tiếp đọc dữ liệu từ Data Node đó
  - **WRITE**
    - Name Node sẽ cần đảm bảo file đó chưa tồn tại,
      nếu đã tồn tại thì sẽ không báo lỗi.
    - Nếu như file chưa tồn tại vì client sẽ được cấp quyền để truy cập vào Data node để ghi dữ liệu
- Nguyên lý cốt lõi của HDFS như sau:
  - Chỉ ghi thêm (Append) giúp giảm chi phí điều khiển tương tranh
  - Phân tán dữ liệu
  - Nhân bản dữ liệu để bảo toàn tính toàn vẹn dữ liệu
  - Có cơ chế chịu lỗi:
    - Các Data Node có thể được backup nhờ sử dụng cơ chế nhân bản dữ liệu
    - Còn các Name Node cũng sẽ có Secondary Name Node để có thể thay thế nếu có lỗi
- Video: [HDFS](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/sfnQg/hdfs)

#### Map Reduce

- **Hadoop MapReduce** cho phép phân tán dữ liệu từ một máy chủ sang nhiều máy con
  Mỗi máy con này sẽ nhận một phần dữ liệu khác nhau và tiến hành xử lý cùng lúc
  Sau đó chúng sẽ báo lại kết quả lên máy chủ
  Máy chủ tổng hợp thông tin lại rồi trích xuất theo như yêu cầu của người dùng
- Các thực thi theo mô hình như vậy giúp tiết kiệm nhiều thời gian xử lý
  và cũng giảm gánh nặng lên hệ thống
- Chức năng của máy chủ là
  - quản lý tài nguyên
  - đưa ra thông báo
  - lịch trình hoạt động cho các Cluster
- Các cluster sẽ thực thi theo kế hoạch được định sẵn
  và gửi báo cáo dữ liệu lại cho máy chủ
  Tuy nhiên đây cũng là điểm yếu của hệ thống này
  Nếu máy chủ bị lỗi thì toàn bộ quá trình sẽ bị ngừng lại hoàn toàn
- Một chương trình Map Reduce có thể được lập trình với nhiều ngôn ngữ khác nhau
  ( Java, C+++, Python, R, Ruby)
  bạn sẽ cần phải cung cấp các hàm Map và Reduce để xác định các công việc phải làm
  hai hàm này được thực thi bởi các tiến trình Mapper và Reducer tương ứng.
- Trong chương trình MapReduce
  - dữ liệu được nhìn nhận như là các cặp khóa - giá trị (key-value)
  - các hàm Map và Reduce nhận đầu vào và trả về đầu ra các cặp (key-value)
- ![MapReduce flow](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L2_2..png?alt=media&token=bd81ffed-555e-4199-8dc7-db177d302175)
- Cơ chế hoạt động của MapReduce sẽ gồm các bước như sau
  - **Input** -> nạp dữ liệu cần xử lý
  - **Split** -> hệ thống sẽ chia dữ liệu thành các mảng cho từng máy
  - **Map** -> thực hiện song song hàm Map cho các mảnh dữ liệu đã chia trước đó
  - **Shuffle** -> sắp xếp và phân chia dữ liệu từ Map đưa vào các Reduce
  - **Reduce** -> thực hiện hàm Reduce cho các dữ liệu đã chia
  - **Output** -> gộp các dữ liệu đầu ra từ bước Reduce để trả về kết quả cuối cùng
- ![How MapReduce works](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L2_3.png?alt=media&token=91fe892c-b905-432e-b4e7-7b0ebdd38784)
- Video: [Map Reduce](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/nb5Lf/intro-to-mapreduce)

#### Others in Hadoop

##### YARN

- YARN : Yet Another Resource Negotiator
- YARN đóng vai trò cấp phát lượng tài nguyên phù hợp cho các ứng dụng khi có yêu cầu
- do Nodes có tài nguyên là
  bộ nhớ và CPU cores
  vậy nên các có cơ chế cấp phát hợp lý
- YARN cung cấp daemons và APIs cần thiết cho việc phát triển ứng dụng phân tán
  đồng thời xử lý và lập lịch sử dụng tài nguyên tính toán (CPU hay memory)
  cũng như giám sát quá trình thực thi các ứng dụng đó
- YARN tổng quát hơn MapReduce thể hệ đầu tiên
  (JobTracker/ TaskTracker)
- Video: [YARN](https://www.coursera.org/learn/big-data-introduction/lecture/Rn7sf/yarn-a-resource-manager-for-hadoop)

##### Hive

- Hive là một nền tảng được sử dụng để phát triển
  các tập lệnh gần giống SQL (HiveSQL)
  để thực hiện các hoạt động MapReduce
  từ đó giúp giảm thời gian phát triển mà vẫn có thể sử dụng được MapReduce
- Trình biên dịch Hive chạy trên các máy client
  giúp chuyển HiveQL script thành MapReduce jobs và đệ trình các công việc này lên cụm tính toán
- Video: [Hive](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/V8YLj/hive)

##### HBase

- HBase là một CSDL cột mở rộng phân tán, lưu trữ trên HDFS
- được xem như là hệ quản trị CSDL của Hadoop
- Dữ liệu được tổ chức về mặt logic là các bảng
  bao gồm rất nhiều dòng và cột
- Ưu điểm
  - Có tính khả mở cao, đáp ứng băng thông ghi dữ liệu tốc độ cao
  - Hỗ trợ hàng trăm ngàn thao tác INSERT mỗi giây (s)
- Nhược điểm

  - Do là NoSQL nên không có ngôn ngữ truy vấn mức cao như SQL và phải sử dụng API để scan/put/get dữ liệu theo khóa

- Video: [HBase](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/pI5Tx/hbase)

### Monitor and process Big Data

#### Big Data system management

##### Data Ingestion - bổ sung dữ liệu

- Quy trình bổ sung dữ liệu cho hệ thống nắm vai trò rất quan trọng
  và cũng tiêu tốn khá nhiều tài nguyên
- Các câu hỏi giúp xác định được quy trình bổ sung dữ liệu phù hợp:
  - Có bao nhiêu Data Source?
  - Độ lớn của dữ liệu?
  - Số Data Source có tăng lên nữa không?
  - Thời gian sẽ cần bổ sung thêm dữ liệu?
  - Bạn sẽ làm gì với các dữ liệu không tốt?
  - Bạn sẽ làm gì nếu lượng dữ liệu quá ít hoặc quá nhiều?
- Video: [Data Ingestion](https://www.coursera.org/learn/big-data-management/lecture/Lnlwf/data-ingestion)

##### Data storage - lưu trữ dữ liệu

- Tương tự với việc bổ sung dữ liệu thì lưu trữ dữ liệu cũng có vai trò quan trọng
- Sẽ có 2 vấn đề ở phần này
  - Cần phân bổ lưu trữ dữ liệu như thế nào
    Bao nhiêu dữ liệu cần lưu trữ và sẽ lưu trữ theo cách nào
    (truy cập trực tiếp hay truy cập qua Internet)
  - Yêu cầu về tốc độ đọc/ ghi dữ liệu như thế nào
- Sau khi có đáp án cho các câu hỏi trên
  thì ta có thể chọn được 1 cách lưu trữ dữ liệu để đáp ứng các yêu cầu đó
- Video: [Data Storage](https://www.coursera.org/learn/big-data-management/lecture/RplBY/data-storage)

##### Data quality - Chất lượng dữ liệu

- có 3 lý do chính mà bạn cần đảm bảo dữ liệu của bạn chất lượng
  - chúng ta sẽ trích xuất thông tin hữu ích từ dữ liệu
    vậy nên nếu ban đầu dữ liệu đã không chất lượng
    thì có thể làm ảnh hưởng đến các phân tích nghiệp vụ sau này
  - Dữ liệu liên quan đến các ngành như y tế, ngân hàng, ...
    nếu có sai sót thì có thể liên quan đến cả các vấn đề pháp lý
  - Nếu dữ liệu chúng ta cung cấp cho khách hàng
    hoặc các bên liên quan không tốt thì có thể làm giảm uy tín
- Video: [Data quality](https://www.coursera.org/learn/big-data-management/lecture/xdrBU/data-quality)

##### Data operations - Các thao tác làm việc với dữ liệu

- Để phù hợp với một yêu cầu nào đó,
  ta cần sử dụng các thao tác để làm việc với dữ liệu
- Các thao tác có thể được chia làm thành 2 loại
  - Thao tác trên 1 data item
  - Thao tác trên 1 tập hợp các dữ liệu
    Thao tác này còn có thể được chia nhỏ hơn thành các loại như sau:
    - Thao tác để chọn một phần dữ liệu (lấy subset)
    - Thao tác để kết hợp các tập hợp dữ liệu (merge)
    - Thao tác tính toán trên tập dữ liệu (đếm dữ liệu,..)
- Video: [Data operations](https://www.coursera.org/learn/big-data-management/lecture/3iZzB/data-operations)

##### Khả năng mở rộng và bảo mật dữ liệu

- Chúng ta có hai cách để mở rộng cho môi trường Big Data
  - **Mở rộng theo chiều dọc**
    - Nâng cấp cấu hình phần cứng cho các máy trên hệ thống
    - Cách này giúp các thao tác được thực hiện tốt hơn
      nhưng đồng thời cũng rất khó triển khai và maintain
  - **Mở rộng theo chiều ngang**
    - Bổ sung thêm các máy mới vào hệ thống
    - Cách này thường được dùng với các hệ thống xử lý phân tán
      tuy có thể hiệu suất sẽ kém hơn chút nhưng lại dễ dàng để triển khai và maintain
      -> Các môi trường Big Data thường sử dụng mở rộng theo chiều ngang
- Ngoài ra, bảo mật cũng là 1 công việc quan trọng
  Một số lưu ý về vấn đề bảo mật
  - Tăng số lượng máy trong hệ thống
    đồng nghĩa với việc quản lý bảo mật khó khăn hơn
  - Các dữ liệu được vận chuyển qua lại giữa các máy
    cũng cần phải đảm bảo tính bảo mật
  - Việc mã hóa và giải mã sẽ tốn nhiều tài nguyên
    nên ta cần phải sử dụng hợp lý
- Video: [Data Scalability & Security](https://www.coursera.org/learn/big-data-management/lecture/JEcdQ/data-scalability-and-security)

##### Tài liệu tham khảo

- Bạn có thể tham khảo thêm các tài liệu dưới đây về việc quản lý Big Data
- Tham khảo: [Energy Data Management Challenges at ConEd](https://www.coursera.org/learn/big-data-management/lecture/r7dn2/energy-data-management-challenges-at-coned)
- Tham khảo: [Gaming Industry Data Management: Q&A with Apmetrix CTO Mark Caldwell](https://www.coursera.org/learn/big-data-management/lecture/uDoPs/gaming-industry-data-management-q-a-with-apmetrix-cto-mark-caldwell)
- Tham khảo: [Flight Data Management at FlightStats: A Lecture by CTO Chad Berkley](https://www.coursera.org/learn/big-data-management/lecture/ITqCN/flight-data-management-at-flightstats-a-lecture-by-cto-chad-berkley)

#### Big Data processing

##### Big Data Pipeline

- Hầu hết các ứng dụng BigData được cấu tạo bởi nhiều thao tác kết hợp lại với nhau
  ta gọi đó là Pipeline
- MapReduce cũng có thể được gọi là 1 **Big Data Pipelines** do gồm các thao tác như Input, Split, Map, Shuffle, Reduce và Output
- Video: [Big Data Processing Pipelines](https://www.coursera.org/learn/big-data-integration-processing/lecture/c4Wyd/big-data-processing-pipelines)

##### Một số thao tác thường dùng trong Big Data Pipelines

- **Map**
  Áp dụng một thao tác nào đó trên toàn bộ phần tử của dữ liệu
- **Reduce**
  Kết hợp các dữ liệu theo các giá trị **key**, đầu vào của thao tác này thường là các dữ liệu có dạng **key-value**
- **Cross/Catesian**
  Áp dụng một thao tác nào đó trên từng cặp dữ liệu từ hai Sets
- **Match/Join**
  Đầu vào của thao tác này thường là các tập dữ liệu có dạng **key-value**
  sau đó áp dụng thao tác nào đó lên những cặp dữ liệu cùng giá trị **key** từ 2 tập đầu vào
- **Co-Group**
  Nhóm các phần tử với nhau
- **Filter**
  Lọc các phần tử thỏa mãn điều kiện
- Video: [Một số thao tác trong Big Data Processing Pipelines](https://www.coursera.org/learn/big-data-integration-processing/lecture/WZPCd/some-high-level-processing-operations-in-big-data-pipelines)

##### Aggregation trong Big Data Pipelines

- **GROUP BY** - chia nhóm các dữ liệu dựa trên một điều kiện nào đó
- Các thao tác tính toán như: **AVERAGE, MIN, MAX, STANDARD DEVIATION**
- Đồng thời, bạn cũng có thể kết hợp tuần tự các Aggregation với nhau cho các thao tác phức tạp hơn
- Video: [Các toán tử Aggregation trong Big Data Processing Pipelines](https://www.coursera.org/learn/big-data-integration-processing/lecture/VDBgw/aggregation-operations-in-big-data-pipelines)

##### Tổng quan Big Data processing

- 1 Hệ thống Big Data Processing sẽ có các tầng như sau:
  - **Data Management and Store**
    Tầng này sẽ chứa các thành phần giúp quản lý và lưu trữ các dữ liệu như HDFS
  - **Data Integration and Processing**
    Tầng này chứa các thành phần để dữ liệu được truy xuất, tích hợp và phân tích
    như YARN, Map Reduce,....
  - **Coordination and Workflow management**
    Tầng này chứa các thành phần để quản lý
    điều phối công việc cho các công cụ của 2 hai tầng còn lại
- Video: [Tổng quan về các hệ thống Big Data Processing](https://www.coursera.org/learn/big-data-integration-processing/lecture/Z1jrr/overview-of-big-data-processing-systems)

## Big Data with Spark

### Spark

- **Apache Spark**
  - là 1 framework mã nguồn mở
  - cho phép bạn xây dựng nhưng mô hình dự đoán nhanh chóng
    với khả năng thực hiện tính toán cùng lúc trên một nhóm các máy tính
    hay trên toàn bộ các tập dữ liệu
    mà không cần thiết phải trích xuất các mẫu tính toán thử nghiệm
  - tốc độ xử lý dữ liệu có được là do khả năng được thực hiện các tính toán
    trên nhiều máy tính khác nhau trên cùng 1 lúc tại bộ nhớ (in-memories)
    hay hoàn toàn trên RAM
- **Spark** được ưa chuộng hơn **Hadoop** trên một số ứng dụng nhờ tốc độ của mình
  (chính xác hơn là cấu trúc tính toán MapReduce trên Hadoop)
- Ưu điểm:
  thay vì lưu dữ liệu và các kết quả trung gian trong quá trình tính toán
  dưới dạng của các file hệ thống như Hadoop Map-Reduce nó lưu
  trong bộ nhớ của các Node
- Nhược điểm:
  không có hệ thống tập tin phân tán vì người ta thường chạy nó trên nền của Hadoop
- Spark cũng rất dễ dàng sử dụng từ Python, Java hoặc Scala
- Spark sẽ gồm các thành phần chính như sau

  - **Spark Core**
    - Được xem là nền tảng và điều kiện cho sự vận hành của các thành phần còn lại
    - Thành phần này đảm nhận vai trò thực hiện các công việc tính toán, xử lý trong bộ nhớ
      và tham chiếu các dữ liệu được lưu trữ bên ngoài
  - **Spark SQL**
    - Thành phần này giúp thực hiện các thao tác trên các Dataframe
      bằng các ngôn ngữ như Java, Scala hay Python thông qua sự hỗ trợ của ngôn ngữ SQL
    - Cung cấp SchemaRDD với mục đích hỗ trợ cho các kiểu **structured data** và **semi-structured data**.
  - **Spark Streaming**
    - Mục đích sử dụng cho thành phần này chính là coi stream là các mini batches
      và thực hiện các kỹ thuật RDD transformation với các dữ liệu này để phân tích stream
      Điều này giúp việc xử lý stream và phát triển lambda architecture
      trở nên dễ dàng bằng cách tận dụng lại các đoạn code được viết để xử lý batch
  - **MLlib**
    - Là một nền tảng học máy
    - Spark MLlib nhanh hơn gấp 9 lần so với phiên bản chạy trên Hadoop
      (theo so sánh từ benchmark) nhờ kiến trúc phân tán dựa trên bộ nhớ
  - **GraphX**
    - Đây là nền tảng xử lý các đồ thị dựa trên Spark
    - Nó cung cấp các API và được sử dụng để diễn tả tất cả các tính toán có trong đồ thị

- Video: [Khái niệm về Apache Spark](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3708614#questions)

- Tham khảo: [Spark 3 có gì mới?](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/17919510#questions)

### Spark Architecture

- Kiến trúc của Spark bao gồm
  - **Driver (trình điều khiển)**
  - **Executor Processes (trình thực thi)**
- _Driver_ tạo ra **_Spark Job_** và **_Spark Context_**
  chia các Job thành các Task có thể chạy song song trong các trình thực thi trên Cluster
  mỗi ứng dụng sẽ có 1 _Spark Context_ điều phối Executor
- **Spark Job**
  là các công việc tính toán mà ứng dụng cần thực hiện
  và có thể được thực thi song song
- **Spark Task**
  thực hiện các Spark Job trên các phân vùng dữ liệu (Partitions) khác nhau
  và cũng có thể được thực thi song song
- ![Apache Spark Architecture](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L4_1.png?alt=media&token=9dce6761-35ca-447c-976f-25454eb47a36)
- **Stages**
  - là một tập hợp các Task được phân tách bằng một lần _Shuffles_ dữ liệu
    - việc Shuffles rất tốn kém
      vì chúng yêu cầu tuần tự hóa dữ liệu, đĩa và mạng I/O
    - Shuffles sẽ cần thiết
      nếu như các thao tác cần dữ liệu từ các phân vùng (Partitions) khác
      ví dụ: ![Shuffle example](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L4_2.png?alt=media&token=734cfe98-f5a4-421a-af8d-c1bd3519476f)
- Driver (chương trình điều khiển) có thể được chạy ở 2 chế độ sau
  - **Client Mode**
    Trình gửi ứng dụng (Application Submitter)
    chẳng hạn như thiết bị đầu cuối của máy người dùng khởi chạy trình điều khiển bên ngoài Cluster
  - **Cluster Mode**
    Chương trình điều khiển được gửi đến và chạy trên một Worker Node có sẵn bên trong Cluster
  - ![driver running modes](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L4_3.png?alt=media&token=613e7952-b1b7-4563-b287-e4f3913f3d7e)
- Video: [Kiến trúc của Apache Spark](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/7D9xA/apache-spark-architecture)
- Tham khảo: [Sự khác biệt giữa SparkSession và SparkContext](https://sparkbyexamples.com/spark/sparksession-vs-sparkcontext/)

#### Cách cài đặt và chạy chương trình Spark

- Cách 1: Cài đặt trực tiếp trên máy tính
  - Video: [Cài đặt môi trường để sử dụng Spark](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/5586888#questions)
  - Tham khảo: [Cài đặt Pyspark qua Pip hoặc Conda](https://spark.apache.org/docs/3.1.1/api/python/getting_started/install.html)
- Cách 2: Sử dụng trên Google Colab
  - Google Colab là một dạng Jupyter Notebook tùy biến cho phép thực thi Python
    trên nền tảng đám mây, được cung cấp Google
  - Sử dụng Google Colab có những lợi ích ưu việt như:
    - sẵn sàng chạy Python ở bất kỳ thiết bị nào có kết nối internet mà ko cần cài đặt
    - chia sẻ và làm việc nhóm dễ dàng, sử dụng miễn phí GPU cho các dự án về AI
- Trong bài Lab này ta dùng Google collab
  - Tham khảo: [Hướng dẫn cài đặt và làm quen Google Colab cơ bản](https://trituenhantao.io/lap-trinh/lam-quen-voi-google-colab/)
  - Tham khảo: [Hướng dẫn sử dụng Google Colab chi tiết](https://thinhvu.com/2021/07/29/huong-dan-su-dung-google-colab-tutorial-101/)
  - Tham khảo: [Cài đặt và sử dụng PySpark trên Google Colab](https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/)
- Sau khi đã cài đặt thành công Spark, ta sẽ có 2 cách để chạy được một Spark Job
  - **Spark Shell**
    - là một trình tương tác giúp ta viết và chạy trực tiếp các câu lệnh PySpark
    - là một công cụ khá hữu ích để làm quen với các Spark API cũng như thực hiện các thao tác phân tích dữ liệu
  - **spark-submit**
    - như tên gọi thì bạn sẽ submit một Spark Job và thực hiện Job đó
    - công việc của các Job này sẽ được khai báo bằng các đoạn code Python
- Hoặc nếu ta sử dụng một thư viện là pyspark
  ta có thể chạy 1 SparkJob bằng cách gọi `python <file-name>.py`
  - Video: [Thực hành: Cài đặt Dataset](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3702584#questions)
  - Video: [Thực hành: Chạy ứng dụng Spark đầu tiên](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3700612#questions)

### Lab 1 - Setup and run Spark

- [Lab1: Cài đặt và sử dụng Spark](https://colab.research.google.com/drive/1JKmBBvT7hv0civmZU__G923H5wKAiJN5?usp=sharing)
- [Hướng dẫn](https://docs.google.com/document/d/19aXDWH86NaVJ-sAWm9YERHN4KT2Pqe70g162_NITXwc/edit?usp=sharing)
- [Bài làm](https://colab.research.google.com/drive/1BRAIycbiRyKvSIf1eiiaM2GiJqxpjTlk?authuser=1#scrollTo=e_fAhSKr9uhE)

### Spark RDD

#### Resilient Distributed Datasets

- RDD là một cấu trúc cơ bản của Spark
- Là một tập hợp bất biến phân tán của một đối tượng
- Mỗi dataset trong RDD được chia ra thành nhiều phân vùng logical
  có thể được tính toán trên các node khác nhau của một cụm máy chủ (cluster)

- RDD có thể chưa bất kỳ kiểu dữ liệu nào của Python, Java hoặc đối tượng Scala
  bao gồm các kiểu dữ liệu do người dùng định nghĩa
- Thông thường RDD chỉ cho phép đọc, phân mục tập hợp của các bản ghi
- RDDs có thể được tạo ra qua điều khiển xác định trên dữ liệu trong bộ nhớ hoặc RDDs
- RDD là một tập hợp có khả năng chịu lỗi mỗi thành phần có thể được tính toán song song
- Chúng ta sẽ có các thao tác có thế được sử dụng trong RDD
  các thao tác này được chia thành 2 loại:
  - **Transformation**
    - Là các thao tác để biến đổi RDD hiện tại thành 1 RDD mới
      các thao tác này cũng có cơ chế **Lazy evaluation**
    - Khi bạn sử dụng các thao tác Transformation
      thì hệ thống sẽ không thực hiện tính toán ngay lập tức
      các thao tác này chỉ được thực thi khi có 1 _Action_ được gọi
  - **Action**
    - Là các thao tác trả về kết quả từ RDD ngay lập tức
- Quản lý các thao tác trong RDD:
  - Chúng ta sẽ sử dụng **DAG (Directed Acyclic Graph)** trong Spark
    Đây là cấu trúc điều phối các yêu cầu tính toán trong mạng các máy tính (cluster)
    giúp lập trình ứng dụng với Spark trở nên đơn giản hơn
  - Thay vì phải liên tục lưu và trải dữ liệu cho các thao tác Map-Reduce
    DAG cho phép các bước tính toán phức tập được thực hiện một cách tuần tự
    Cơ chế hoạt động của DAG sẽ như sau
  - Spark sẽ tạo 1 DAG mới khi tạo mới 1 RDD
  - Khi có 1 thao tác Transformation, DAG sẽ được cập nhật và lúc này DAG sẽ được trỏ tới RDD mới
  - Khi có 1 Action được gọi, Driver sẽ tiến hành các thao tác tính toán đã được lưu trong DAG và trả về kết quả
- Video: [Giới thiệu về RDD](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/Z6cIK/rdds-in-parallel-programming-and-spark)

#### Transformations and Actions trong RDD

##### Transformations

- `dictinct`
  loại bỏ trùng lắp trong RDD
- `filter`
  tương đương với việc sử dụng where trong SQL - các record trong RDD thỏa điều kiện
  có thể cung cấp một hàm phức tạp sử dụng để filter các record cần thiết
  như trong Python, ta có thể sử dụng hàm lambda để truyền vào filter
- `map`
  thực hiện một công việc nào đó trên toàn bộ RDD
  trong Python sử dụng lambda với từng phẩn tử để truyền vào map
- `flatMap`
  cung cấp 1 hàm đơn giản hơn hàm map
  yêu cầu output của map phải là 1 structure có thể lặp và mở rộng được
- `sortBy`
  mô tả một hàm để trích xuất dữ liệu từ các object của RDD và thực hiện sort được từ đó
- `randomSplit`
  nhận một mảng trọng số và tạo một random seed
  tách các RDD thành một mảng các RDD do số lượng chia theo trọng số
- `reduceByKey`
  gộp các giá trị theo các key giống nhau
- `groupByKey`
  nhóm các phần tử có key giống nhau
- `sortByKey`
  sắp xếp các dữ liệu của RDD dựa trên các key
- `keys()` hoặc `values()`
  tạo 1 RDD chỉ gồm các key hoặc value
- ví dụ và tài liệu tham khảo
  - Video: [Các hàm xử lý Key/Value trong RDD](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3710440#questions)
  - Tham khảo: [Thực hành: Average Friends by Age](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3710452#questions)
  - Video: [Lọc các dữ liệu trong RDD](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3719528#questions)
  - Tham khảo: [Thực hành: Minimum Temperature](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3719534#questions)
  - Video: [flatMap() trong RDD](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3723874#questions)

##### Actions

- `reduce`
  thực hiện hàm reduce trên RDD để thu về 1 giá trị duy nhất
- `count`
  đếm số dòng trong RDD
- `countByValue`
  đếm số giá trị của RDD chỉ sử dụng nếu map kết quả nhỏ
  vì tất cả dữ liệu sẽ được load lên memory của driver để tính toán
  chỉ nên sử dụng trong tình huống dòng nhỏ và số lượng item khác cũng nhỏ
- `max()` và `min()`
  lần lượt lấy giá trị lớn nhất và nhỏ nhất của dataset
- `collect`
  trả về giá trị của RDD
- tài liệu tham khảo:
  - Video: [Sử dụng Regex cho bài toán Word Count](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3723878#questions)
  - Video: [Sắp xếp lại các dữ liệu từ bài toán Word Count](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3723880#overview)
  - Tham khảo: [Các thao tác khác trong RDD.](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html)

### Lab2 - Using RDD Exercise

- [Lab 2.1: Average Friends by Age](https://colab.research.google.com/drive/1pXHwWFOQmh_am6PX9UibL1wA9PlsrU-j?usp=sharing)
- [Lab 2.2: Minimum Temperature by Location](https://colab.research.google.com/drive/1JseilAKLLpzel8ix4yXJBsLKR-dYuIxa?usp=sharing)
- [Hướng dẫn cập nhật Spark](https://docs.google.com/document/d/19aXDWH86NaVJ-sAWm9YERHN4KT2Pqe70g162_NITXwc/edit?usp=sharing)

### SparkSQL, Data-frames, Datasets

#### Data-frames, Datasets

- Dataframe và Datasets là 2 khái niệm rất quan trọng trong SparkSQL
  Spark SQL là 1 trong những thành phần chính của Spark
- **DataFrame**
  - là 1 API bậc cao hơn RDD được Spark giới thiệu vào năm 2013 (từ Apache Spark 1.3)
  - tương tự RDD, dữ liệu trong DataFrame cũng được quản lý theo kiểu phân tán
    và không thể thay đổi (immutable distributed).
  - tuy nhiên dữ liệu trong DataFrame được sắp xếp theo các cột,
    tương tự như trong các CSDL
  - DataFrame được phát triển để giúp người dùng có thể dễ dàng thực hiện được
    các thao tác xử lý dữ liệu cũng như làm tăng đáng kể hiệu quả xử lý của hệ thống
  - Ta có thể đọc, ghi dữ liệu với Dataframe ở các dạng file như JSON, CSV hoặc Hive, ...
  - Trong việc xử lý dữ liệu chung ta cũng có thể sử dụng các câu lệnh SQL
    để truy vấn dữ liệu nhanh hơn cũng như dễ dàng hơn các truy vấn phức tạp
    ![dataframe spark sql](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L6_1.png?alt=media&token=eaf3ce5a-1dfc-4df7-adea-3b520783e633)
  - Hoặc sử dụng các hàm được xây dựng sẵn trong Dataframe API
    Tuy nhiên, ở bước cuối cùng thì các thuật toán này vẫn được chạy trên RDD
    mặc dù người dùng chỉ tương tác với DataFrame
    ![dataframeAPI spark sql](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L6_2.png?alt=media&token=85f40fda-d2ec-418e-9c23-8e9831f17f1e)
- **Datasets**
  - Lên đến Spark 2.0 thì Data Frame và Dataset được hợp nhất thành 1
    nên ở môn học này chúng ta sẽ thống nhất chỉ sử dụng Dataset
- Video: [Giới thiệu về SparkSQL](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/5586908#overview)

#### Data Sources for Big Data

- Khi hoạt động, _Data Processing Engine_ như Spark sẽ cần đọc dữ liệu từ 1 Data Source nào đó
  và cũng sẽ cần lưu dữ liệu sau khi được xử lý vào một Data Sink nào đó
  Các nguồn dữ liệu này có thể được chia ra làm hai loại

  - **External**
    - là các nguồn dữ liệu nằm ngoài hệ thống BigData
    - ví dụ như một Database từ hệ thống khác, hoặc là các file dữ liệu từ hệ thống
    - chúng ta sẽ có 1 số loại Extenal Data Source như sau
      - JDBC Datasource: SQL Server, Oracle, PostgresSQL, ...
      - NoSQL Data System: MongoDB, Cassandra, ...
      - Cloud Data Warehouse
      - Stream Integrators: Kafka,...
    - Để tiếp cận ta thường có 2 cách:
      - Lấy dữ liệu từ Source và lưu lại ở hệ thống BigData, sau đó mới bắt đầu xử lý
        Cách này sẽ phù hợp với các trường hợp Batch processing
        do có thể đảm bảo về các yếu tố như hiệu suất, tính bảo mật, ...
      - Truy cập trực tiếp Datasource, sau đó lấy và xử lý dữ liệu
        Cách này phù hợp với các trường hợp Stream Processing
  - **Internal**
    - Ngược lại với External, đây là nguồn dữ liệu trực tiếp bên trong hệ thống BigData
      như **HDFS**, **AWS S3**, ...
      và để lấy dữ liệu thì bạn sẽ truy cập trực tiếp vào dữ liệu

- Video: [Nguồn dữ liệu cho Big Data](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20434107#overview)

#### Read & Write data

##### DataFrameReader API

- Để đọc dữ liệu từ các file, Spark cung cấp DataFrameReader API
- ![ví dụ](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L6_3.png?alt=media&token=81676d2c-178a-4561-8290-abd66b042a01)
- ta cần cung cấp một số thông tin cơ bản để thực hiện việc đọc dữ liệu như
  - **format** : loại file cần đọc
  - **path** : đường dẫn đến file cần đọc
  - **mode** : cơ chế đọc file
    do DataFrame làm việc với các dữ liệu có cấu trúc
    nên đôi khi trong lúc đọc file sẽ có một số phần tử bị lỗi
    (thiếu trường dữ liệu, sai kiểu dữ liệu,..)
    và các cơ chế đọc file sẽ có từng cách xử lý khác nhau
    có 3 MODE đọc file:
    - **Permissive** : tất cả các trường được đặt thành null
      và các bản ghi bị hỏng được đặt trong một cột được gọi là \_corrupt_record.
    - **DropMalformed** : xóa các hàng bị lỗi
    - **FailFast** : đưa ra lỗi cho hệ thống khi có hàng bị lỗi
  - **schema** : cấu trúc của dữ liệu ta muốn đọc
    hoặc có thể để hệ thống tự thiết lập nếu như dữ liệu có schema đơn giản
- video: [Spark DataFrameReader API](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20434109#overview)
- video: [Thực hành: Đọc dữ liệu từ file JSON, CSV,...](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20434113#overview)

##### Spark dataschema

- Chúng ta có thể cung cấp Schema để Spark có thể nắm được cấu trúc của dữ liệu
- Spark sẽ có các kiểu dữ liệu như sau: ![scala types](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L6_4.png?alt=media&token=20f63cfb-b18f-4442-a93f-dd0c4d403fd2)
- Chúng ta sẽ có 2 cách khai báo Schema:
  - Cách 1: Khai báo dưới dạng StructType
    ![structType declaration](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L6_5.png?alt=media&token=cd3da1ea-4a1b-4bb8-98c5-cef6e88874b0)
  - Cách 2: Khai báo dưới dạng DDL String
    ![schemaDDL declaration](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L6_6.png?alt=media&token=3c17ba54-bf79-4921-8680-1b619209cfc7)
- Video: [Tạo Spark DataFrame Schema](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20434117#overview)

##### DataFrameWriter API

- Tương tự với việc đọc dữ liệu thì Spark SQL cũng cung cấp cho chúng ta DataFrameWriter API
  để ghi dữ liệu ra các Sink, cách sử dụng DataFrameWriter như sau
  ![DataFrameWriter](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L6_8.png?alt=media&token=9b4dde07-7e38-49e1-84d9-a180fb54797c)
- Ta cũng sẽ cần khai báo 1 tham số để thiết lập cho việc ghi dữ liệu
- Video: [Spark DataFrameWriter API](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20434121#overview)
- Video: [Thực hành: Lưu dữ liệu ra file](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20434125#overview)

### Lab3 - Read write operators with Dataframe

- [Gdrive](https://colab.research.google.com/drive/1NMdi4R255huTfbr2g9p6fxTnQTRvPlRH?usp=sharing)

### SparkSQL and SparkSQL Table

#### SparkSQL manual

- Ngoại trừ sử dụng các hàm có sẵn để tương tác với dữ liệu
  thì Spark cũng cung cấp cơ chế sử dụng câu lệnh SQL để truy vấn dữ liệu
  Để sử dụng cơ chế này, bạn sẽ cần lưu trữ lại Dataframe đó dưới dạng 1 Hive Table trong Spark Session
  (khi session này kết thúc thì bảng đó cũng sẽ bị xóa)
  và sau đó có thể sử dụng các câu lệnh SQL để truy vấn
  ví dụ như sau:
  ```python
  surveyDF.createOrReplaceTempView("survey_tbl")
  countDF = spark.sql("select Country, count(1) as count from survey_tbl where Age<40 group by Country")
  ```
- Video: [Sử dụng SparkSQL](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20399111#overview)
- Để có thể thực hiện được các câu lệnh SQL, cơ chế này sẽ gồm 4 bước như sau và sẽ được thực hiện thông qua **Spark SQL Engine**:
  - **Analysis**:
    Spark sẽ đọc và tạo nên 1 Abstract Syntax Tree
    từ đó sẽ lấy được các thông tin như tên bảng, tên cột,...
  - **Logical Optimization**:
    Spark sẽ tối ưu hóa lại các truy vấn để tiết kiệm thời gian thực thi
  - **Physical Planning**:
    Spark sẽ lựa chọn ra phương án tối ưu phù hợp nhất để thực thi
  - **Code Generation**:
    Chuyển hóa truy vấn về các câu lệnh Java rồi thực hiện
- Video: [Spark SQL engine và Catalyst optimizer](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20399117#overview)
- Video: [Tối ưu hóa truy vấn trong SparkSQL](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/DFpdP/catalyst-and-tungsten)

#### Spark Databases and Tables

- Trong spark cũng có _Database_ trong đó, vậy nên có thể tạo các Database
  và từ đó tạo các Table và View ở trong chính Spark.
  Một table ở trong Spark sẽ có 2 phần:
  - **Table Data**:
    được lưu trữ dưới dạng Data File ở trong hệ thống lưu trữ phân tán
  - **Table Metadata**:
    hay còn được gọi là catalog
    lưu các thông tin về bảng và dữ liệu của bảng đó như Schema: tên bảng, phân vùng,...
    Các dữ liệu này có thể được lưu theo Spark Session
    hoặc ở Hive
- _Spark table_ cũng được chia thành 2 loại:
  - **Managed Tables**:
    Dữ liệu ở Table sẽ được lưu ở 1 đường dẫn là **spark.sql.warehouse.dir**
    (do Cluster Admin quy định)
    Và khi ta xóa Table này đi thì Spark cũng sẽ xóa cả Metadata và dữ liệu của bảng
  - **Unmanaged Tables**:
    Khi tạo bảng này thì ta sẽ cần khai báo thêm cả về nơi mà Table Data sẽ được lưu trữ
    (có thể là một Data Store ở bên ngoài)
    lúc này Spark sẽ chỉ tạo Table Metadata để lưu trữ các thông tin về bảng.
    Khi ta xóa Table này thì Spark cũng chỉ xóa Metadata chứ không xóa dữ liệu
- Video: [Spark Databases và Tables](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20434129#overview)
- Để tạo một Managed Tables, ta có thể sử dụng câu lệnh như sau:
  ```python
  flightTimeParquetDF.write \
    .mode("overwrite")
    .saveAsTable("flight_data_tbl")
  ```
- Video: [Làm việc với SparkSQL Table](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20434133#overview)
- Video: [Ví dụ: Sử dụng Spark SQL](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/5586918#overview)

### Lab 4 - Using SparkSQL Table

- Lab 4.1 - [HelloSparkSQL](https://colab.research.google.com/drive/13_HlauTB5JYyXkIlBzloO_nl6HJQaLfF?usp=sharing)
- Lab 4.2 - [SparkSQLTableDemo](https://colab.research.google.com/drive/1pa4x9eoH4LVJ2rquNWeuT_gdrnWOFoUz?usp=sharing)

### Data transformation with Spark

#### Row and Column

- sau khi đã đọc được dữ liệu thì chúng ta sẽ cần thực hiện các phép chuyển đổi dữ liệu
- khi đọc dữ liệu, ta có thể lưu chúng dưới 2 dạng khác nhau
  - **dataframe**
  - **database table**
- 1 số thao tác phổ biến về việc xử lý dữ liệu trên Dataframe:
  - **hợp nhất các Dataframe( JOIN, UNION)**
  - **Tổng hợp lại dữ liệu từ nhiều Dataframe(joining, windowing, rollups)**
  - **Sử dụng các hàm và các Transformation có sẵn**
  - **Sử dụng các hàm tự định nghĩa**
- udemy: [Giới thiệu về Data Transformation](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20495288#overview)
- do dataframe sẽ lưu trữ các dữ liệu có cấu trúc theo các cột và hàng
  nên các thao tác cũng được chia ra để sử dụng cho các cột và hàng
  các thao tác với hàng sẽ biển đổi từng hàng trong Dataframe thành 1 hàng mới
  vd: ![dataframe rows](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L6_7.png?alt=media&token=a5b86e20-40f3-4a97-b980-80bfb40a741b)
- udemy: [Làm việc với dataframe rows](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20495330#overview)
- video tham khảo: [Dataframe Rows và Unit Testing](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20554784#overview)
- một ví dụ điển hình cho việc biến đổi các Row đó là để chuyển các dữ liệu
  không cấu trúc thành các dữ liệu có cấu trúc.
  giả sử ta muốn lấy các dữ liệu từ log file vô cùng lộn xộn chứ không được
  sắp xếp theo các cột và hàng.
  ví dụ ta có log file như sau:
  ```
  83.149.9.216 - - [17/May/2015:10:05:03 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
  83.149.9.216 - - [17/May/2015:10:05:43 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png HTTP/1.1" 200 171717 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
  83.149.9.216 - - [17/May/2015:10:05:47 +0000] "GET /presentations/logstash-monitorama-2013/plugin/highlight/highlight.js HTTP/1.1" 200 26185 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
  83.149.9.216 - - [17/May/2015:10:05:12 +0000] "GET /presentations/logstash-monitorama-2013/plugin/zoom-js/zoom.js HTTP/1.1" 200 7697 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
  83.149.9.216 - - [17/May/2015:10:05:07 +0000] "GET /presentations/logstash-monitorama-2013/plugin/notes/notes.js HTTP/1.1" 200 2892 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
  83.149.9.216 - - [17/May/2015:10:05:34 +0000] "GET /presentations/logstash-monitorama-2013/images/sad-medic.png HTTP/1.1" 200 430406 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
  ```
- nếu như bạn cần trích xuất các thông tin như thời gian, IP,...
  thì chúng ta sẽ cần phải sử dụng các Row Transformation
  để xử lý từng hàng dữ liệu.
  Spark cũng cung cấp cho chúng ta nhiều hàm hỗ trợ cho việc xử lý dữ liệu,
  ta có thể tham khảo các hàm đó và xem ví dụ ở các video sau
- udemy: [Dataframe Row và dữ liệu phi cấu trúc](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20585510#overview)
- [Spark Dataframe API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html?highlight=dataframe)
- [Spark Function](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html)
- Tương tự với các hàng, Spark cũng cung cấp các hàm để chuyển đổi cho các cột
  Chúng ta sẽ có 2 thao tác chính như sau:
  - Tạo một con trỏ tới các cột trong Dataframe
  - Tạo các biểu thức để biến đổi các dữ liệu theo từng cột
- Với mỗi thao tác này thì chúng ta sẽ có 2 các làm là thông qua:
  - **Column String**
    - là cách chúng ta sử dụng một chuỗi để khai báo tên cột muốn lấy
      hoặc là khai báo biểu thức mà chúng ta muốn dùng cho cột đó
    - ví dụ: sử dụng _Column String_ để tạo con trỏ đến các cột:
      ```python
      airlinesDF.select("Origin", "Dest", "Distance").show(10)
      ```
    - ví dụ: sử dụng _Column String_ để khai báo các biểu thức biến đổi
      ```python
      airlinesDF.selectExpr("Origin", "Dest", "Distance", "to_date(concat(Year,Month,DayOfMonth),'yyyyMMdd') as FlightDate").show(10)
      ```
  - **Columns Object**
    - cũng giống như String nhưng thay vì dùng chuỗi thì bạn sẽ dùng các hàm
      hoặc các đoạn code Python để thực hiện việc tạo con trỏ hoặc khai báo
      biểu thức
    - ví dụ: sử dụng _Column Object_ để tạo con trỏ đến các cột:
      ```python
      from pyspark.sql.functions import *
      airlinesDF.select(column("Origin"), col("Dest"), "Distance").show(10)
      ```
    - ví dụ: sử dụng _Column Object_ để khai báo các biểu thức biến đổi
      ```python
      airlinesDF.select("Origin", "Dest", "Distance")
      to_date(concat("Year","Month","DayofMonth"),"yyyyMMdd").alias("FlightDate")
      ```
- udemy: [Làm việc với Dataframe Columns](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20601288#overview)

#### Data transformation by other ways

- Ngoài sử dụng các hàm có sẵn trong Spark
  chúng ta có thể tự định nghĩa để xử lý dữ liệu được gọi là UDF **(User Defined Function)**
  Cách này sẽ thuận tiện hơn nếu bạn muốn thực hiện các xử lý phức tạp.
  Để sử dụng được cơ chế này bạn sẽ cần 3 bước:
  - Khai báo hàm bạn sẽ sử dụng để xử lý dữ liệu
  - Đăng ký hàm đó thành 1 UDF
  - Sử dụng hàm
- UDF cũng sẽ hỗ trợ 2 cách thức là qua **String Expression** hoặc **Object Expression**
  với mỗi cách thức sử dụng này thì việc đăng ký và sử dụng UDF cũng sẽ khác nhau
- Để sử dụng _Object Expression_ ta có ví dụ như sau:
  ```python
  # Register UDF
  parse_gender_udf = udf(parse_gender, returnType=StringType())
  # Use UDF
  survey_df2 = survey_df.withColumn("Gender", parse_gender_udf("Gender"))
  ```
- Để sử dụng _String Expression_ ta cũng có ví dụ như sau:
  ```python
  # Register UDF
  spark.udf.register("parse_gender_udf", parse_gender, StringType())
  # Use UDF
  survey_df3 = survey_df.withColumn("Gender", expr("parse_gender_udf(Gender)"))
  ```
- Udemy: [Sử dụng UDF](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20655744#overview)
- chúng ta có một số tip và code để thực hiện 1 số thao tác với Dataframe nhanh hơn từ video sau: [Misc Transformation](https://funix.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20702078#overview)
  - Cách tạo Dataframe nhanh nhất
  - Tạo trường id có thể tự động tăng
  - Sử dụng Case When Then Transformation
  - Chuyển đổi kiểu dữ liệu của một cột
  - Thêm/xóa các cột
  - Loại có các hàng trùng nhau
  - Sắp xếp lại Dataframe

## Apache Airflow
