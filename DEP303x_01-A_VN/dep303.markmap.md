# Big Data with Spark

## Lab list

- [Lab 1]
- [Lab 2]
- [Lab 3]
- [Lab 4]
- [Lab 5]
- [Lab 6]
- [Lab 7]
- [Lab 8]
- [Lab 9]
- [Lab 10]
- [Assessment 1]
- [Lab 11]
- [Lab 12]
- [Lab 13]
- [Assessment 2]

## Big Data and Hadoop

### Big Data

#### Big Data concept

- Big Data là thuật ngữ
  dùng để chỉ 1 tập hợp dữ liệu rất lớn và phức tạp
  đến nỗi những công cụ, ứng dụng xử lý dữ liệu truyền thống
  không thể thu thập, quản lý và xử lý dữ liệu trong một khoản thời gian hợp lý
- Những tập hợp dữ liệu lớn này có thể bao gồm
  - các dữ liệu có cấu trúc **(structured data)**
  - các dữ liệu không cấu trúc **(unstructured data)** như video, file doc,...
  - các dữ liệu bán cấu trúc **(semi structured data)**
- Mỗi tập hợp có chút khác biệt
- Vòng đời của Big Data sẽ diễn ra như sau
  - **Business Case**
    Ở bước này, chúng ta sẽ quyết định các định dạng dữ liệu nào được thu thập trên các yêu cầu nghiệp vụ
  - **Data Collection**
    Lúc này dữ liệu sẽ được thu thập và lưu trữ phân tán thông qua HDFS
  - **Data Modelling**
    Để đảm bảo dữ liệu được lưu trữ đầy đủ, ta cần tạo ra các data model để lưu trữ và xác định mối quan hệ giữa các dữ liệu với nhau (Map Reduce và YARN)
  - **Data Processing**
    Sau khi được mô hình hóa, dữ liệu sẽ cần được xử lý để chỉ lấy các thông tin cần thiết (Spark, Pig, Hive, ...)
  - **Data Visualization**
    Dữ liệu cần được trực quan hóa thành các biểu đồ để người dùng có thể sử dụng dữ liệu cho các vấn đề nghiệp vụ
  - ![Big data life cycle](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_1..png?alt=media&token=b23caf48-e2b9-4e0e-a912-ca9ad5d1f4ff)
- Ngoài ra, Big Data cũng có các **đặc trưng (5V)** như sau:
  - **Volume (khối lượng)**
    Khối lượng của dữ liệu Big Data đang tăng lên mạnh mẽ từng ngày
  - **Velocity (tốc độ)**
    Với sự ra đời của các kỹ thuật, công cụ, ứng dụng lưu trữ
    nguồn dữ liệu lớn liên tục được bổ sung với tốc độ nhanh chóng
  - **Veriety (đa dạng)**
    Sự đa dạng của dữ liệu đến từ nhiều nguồn khác nhau
    như từ các thiết bị cảm biến, thiết bị di động, hay thông qua các trang mạng xã hội
  - **Veracity (tin cậy)**
    Bằng những phương tiện truyền thông xã hội bùng nổ như hiện nay
    và sự gia tăng mạnh mẽ về tương tác và chia sẻ của người dùng
    bức tranh toàn cảnh về độ chính xác cũng như tin cậy của thông tin
    ngày càng trở nên hỗn loạn và khó khăn
    Vấn đề phân tích và loại bỏ dữ liệu thiếu chính xác
    đang là một trong những vấn đề lớn của Big Data
  - **Value (giá trị)**
    Đây là đặc trưng **quan trọng nhất** bởi các thông tin đang tiềm ẩn
    trong bộ dữ liệu khổng lồ được trích xuất
    để phục vụ cho việc phân tích, dự báo
    Những thông tin này có thể được sử dụng trong rất nhiều lĩnh vực
    như kinh doanh, nghiên cứu khoa học, y học, vậy lý,...
    giúp giải quyết những vấn đề mà cuộc sống hiện đại đặt ra
- Videos:
  - [Big Data là gì?](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/24c4V/what-is-big-data)
  - [Tổng quan về Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/yqVpL/beyond-the-hype)
  - [Ảnh hưởng của Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/wLFkJ/impact-of-big-data)
  - [Nên sử dụng Big Data trong trường hợp nào?](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/570eD/big-data-use-cases)
- Ngoài ra, do dữ liệu của chúng ta sẽ được đến từ nhiều nguồn
  và đồng thời cũng sẽ ở nhiều dạng khác nhau
  Vậy nên chúng ta sẽ cần kết hợp các dữ liệu đó lại
  để đồng nhất với nhau và giúp dữ liệu giá trị hơn và liên kết hơn
  Công việc này sẽ bao gồm:
  - Làm giảm độ phức tạp của dữ liệu
  - Tăng mức độ phù hợp của dữ liệu
  - Chuyển đổi để các dữ liệu đồng nhất với nhau
  - Video: [Tích hợp dữ liệu ở các dạng khác nhau](https://www.coursera.org/learn/big-data-introduction/lecture/Ejk8J/the-key-integrating-diverse-data)

#### Ecosystems and tools for Big Data

- Chúng ta có hai cách để có thể xử lý các dữ liệu:
  - **Xử lý tuần tự (Linear processing):**
    - Các công việc xử lý dữ liệu sẽ được thực hiện tuần tự.
    - Các công việc trước cần phải hoàn thành thì mới có thể sang công việc tiếp theo
    - Nếu có lỗi xảy ra ở một bước đó thì các công việc sẽ phải
      dừng lại, hoặc chạy lại hết từ đầu sau khi xử lý các lỗi
  - **Xử lý song song (Parallel processing):**
    - Các công việc được xử lý song song, đọc lập với nhau
    - Nếu có 1 công việc bị lỗi thì cũng sẽ không ảnh hưởng
      đến các công việc còn lại và có thể dễ dàng thực hiện lại
  - ![linear & parallel processing](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_2..png?alt=media&token=0fdc5f0e-46e1-4adb-8637-717184fe9665)
- Có thể thấy rằng việc xử lý dữ liệu song song
  sẽ phù hợp hơn với Big Data nhờ vào một số ưu điểm sau:
  - Tiết kiệm thời gian xử lý dữ liệu
  - Tiết kiệm được các tài nguyên tính toán hơn cho các Node
  - Dễ dàng mở rộng hơn nếu dữ liệu nhiều hơn
- Với Big Data, để xử lý dữ liệu song song thì
  chúng ta sẽ chia hệ thống thành nhiều máy **(Cluster)** khác nhau
  mỗi máy có thể đảm nhiệm vai trò lưu trữ, tính toán hoặc cả hai
  Sau đó, chúng ta cần xử lý dữ liệu thì sẽ chạy song song với các Cluster khác nhau
  Kiến trúc này giúp cho dữ liệu có thể được xử lý song song và độc lập với nhau
- ![parallel processing clusters](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_3..png?alt=media&token=1fcea8af-5bd7-48c2-8db1-6b1da86e7fa6)
- Ngoài ra, kiến trúc này cũng giúp dễ dàng mở rộng theo chiều ngang
  tức là khi ta cần một tài nguyên lớn hơn để tính toán thì chỉ việc tạo thêm nhiều Cluster
  chứ không cần phải nâng cấp cấu hình cho các Cluster cũ
  Đồng thời cũng giúp tính bảo toàn tính toàn vẹn dữ liệu thông qua cơ chế **Fault tolerance**
  dữ liệu sẽ được tạo thành nhiều bản sao ở các Cluster khác nhau
  khi 1 Cluster gặp sự cố gây ra mất mát dữ liệu thì ta có thể dễ dàng backup lại
  Ta sẽ được tìm hiểu kỹ hơn ở bài về Hadoop HDFS
- [parallel processing clusters problem](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_4..png?alt=media&token=544bc364-9996-4696-ae90-fd241d154c2e)
- Video: [Xử lý dữ liệu song song và Scaling](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/ix1eD/parallel-processing-scaling-and-data-parallelism)

- Các công cụ trong hệ sinh thái Big Data sẽ được chia thành các công cụ như sau:
  - **Data Technologies**
    Sử dụng để xử lý, chia sẻ các dữ liệu ở định dạng khác nhau và giúp dữ liệu phân tán
  - **Analytics và Visualization**
    Giúp trực quan hóa và phân tích dữ liệu
  - **Business Intelligence**
    Giúp chuyển hóa các dữ liệu để dễ dàng truy cập, phục vụ cho các vấn đề nghiệp vụ
  - **Cloud Provider**
    Cung cấp các dịch vụ dưới dạng Cloud để dễ dàng mở rộng hơn
  - **NoSQL Database**
    Cơ sở dữ liệu NoSQL để lưu trữ dữ liệu
  - **Programming Tools**
    Các công cụ để xử lý các công việc đòi hỏi logic phức tạp trong vòng đời của Big Data
- Và đồng thời sẽ có những công cụ mã nguồn mở giúp chúng ta thao tác với Big Data dễ dàng hơn
- Video: [Hệ sinh thái và các công cụ về Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/WyhaU/big-data-tools-and-ecosystem)
- Video: [Các công cụ mã nguồn mở của Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/Fk5f4/open-source-and-big-data)

### Hadoop

#### Hadoop concepts

- Hadoop là một dạng Apache Framework
- Apache Hadoop là một mã nguồn mở cho phép sử dụng các distributed processing (ứng dụng phân tán) để quản lý và lưu trữ những tệp dữ liệu lớn
- Hadoop áp dụng mô hình MapReduce trong hoạt động xử lý Big Data
- Hadoop có những ưu điểm sau:
  - Cho phép ng dùng nhanh chóng viết và kiểm tra các hệ thống phân tán
    Đây là cách hiệu quả cho phép phân phối dữ liệu và công việc xuyên suốt các Cluster nhờ vào cơ chế xử lý song song của các lỗi CPU
  - Hệ thống Hadoop thiết kế sao cho các lỗi xảy ra trong hệ thống
    được xử lý tự động, không ảnh hưởng đến các ứng dụng phía trên
  - Có thể phát triển lên nhiều server với cấu trúc master-slave
    để đảm bảo thực các công việc linh hoạt và không bị ngắt quãng
    do chia nhỏ công việc cho các server slave được điều khiển bởi server master
  - Có thể tương thích trên mọi nền tảng như Window, Linux, MacOS do được tạo từ Java
- Tuy nhiên Hadoop vẫn có những khuyết điểm như:
  - thiết bị lưu trữ tốc độ chậm
  - máy tính thiếu tin cậy
  - lập trình song song phân tán không dễ dàng
- Video: [Giới thiệu về Hadoop](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/1sOjx/introduction-to-hadoop)
- Hadoop gồm 4 thành phần chính:
  - **Hadoop Common**
    Đây là các thư viện và tiện ích cần thiết của Java để các module khác sử dụng.
    Những thư viện này cung cấp hệ thống file và lớp OS trừu tượng,
    đồng thời chứa các mã lệnh Java để khởi động Hadoop.
  - **Hadoop YARN**
    Đây là framework để quản lý tiến trình và tài nguyên của các cluster.
  - **Hadoop Distributed File System (HDFS)**
    Đây là hệ thống file phân tán cung cấp truy cập thông lượng cao
    cho ứng dụng khai thác dữ liệu.
  - **Hadoop MapReduce**
    Đây là hệ thống dựa trên YARN dùng để xử lý song song các tập dữ liệu lớn.
- Hiện nay Hadoop đang ngày càng được mở rộng cũng như
  được nhiều framework khác hỗ trợ như Hive, Hbase, Pig.
  Tùy vào mục đích sử dụng mà ta sẽ áp dụng framework phù hợp
  để nâng cao hiệu quả xử lý dữ liệu của Hadoop.

- Video: [Hệ sinh thái Hadoop](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/g7y7n/hadoop-ecosystem)

#### HDFS

#### Map Reduce

#### Others in Hadoop

## Big Data with Spark

## Apache Airflow
