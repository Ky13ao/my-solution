# Big Data with Spark

## Lab list

- [Lab 1]
- [Lab 2]
- [Lab 3]
- [Lab 4]
- [Lab 5]
- [Lab 6]
- [Lab 7]
- [Lab 8]
- [Lab 9]
- [Lab 10]
- [Assessment 1]
- [Lab 11]
- [Lab 12]
- [Lab 13]
- [Assessment 2]

## Big Data and Hadoop

### Big Data

#### Big Data concept

- Big Data là thuật ngữ
  dùng để chỉ 1 tập hợp dữ liệu rất lớn và phức tạp
  đến nỗi những công cụ, ứng dụng xử lý dữ liệu truyền thống
  không thể thu thập, quản lý và xử lý dữ liệu trong một khoản thời gian hợp lý
- Những tập hợp dữ liệu lớn này có thể bao gồm
  - các dữ liệu có cấu trúc **(structured data)**
  - các dữ liệu không cấu trúc **(unstructured data)** như video, file doc,...
  - các dữ liệu bán cấu trúc **(semi structured data)**
- Mỗi tập hợp có chút khác biệt
- Vòng đời của Big Data sẽ diễn ra như sau
  - **Business Case**
    Ở bước này, chúng ta sẽ quyết định các định dạng dữ liệu nào được thu thập trên các yêu cầu nghiệp vụ
  - **Data Collection**
    Lúc này dữ liệu sẽ được thu thập và lưu trữ phân tán thông qua HDFS
  - **Data Modelling**
    Để đảm bảo dữ liệu được lưu trữ đầy đủ, ta cần tạo ra các data model để lưu trữ và xác định mối quan hệ giữa các dữ liệu với nhau (Map Reduce và YARN)
  - **Data Processing**
    Sau khi được mô hình hóa, dữ liệu sẽ cần được xử lý để chỉ lấy các thông tin cần thiết (Spark, Pig, Hive, ...)
  - **Data Visualization**
    Dữ liệu cần được trực quan hóa thành các biểu đồ để người dùng có thể sử dụng dữ liệu cho các vấn đề nghiệp vụ
  - ![Big data life cycle](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_1..png?alt=media&token=b23caf48-e2b9-4e0e-a912-ca9ad5d1f4ff)
- Ngoài ra, Big Data cũng có các **đặc trưng (5V)** như sau:
  - **Volume (khối lượng)**
    Khối lượng của dữ liệu Big Data đang tăng lên mạnh mẽ từng ngày
  - **Velocity (tốc độ)**
    Với sự ra đời của các kỹ thuật, công cụ, ứng dụng lưu trữ
    nguồn dữ liệu lớn liên tục được bổ sung với tốc độ nhanh chóng
  - **Veriety (đa dạng)**
    Sự đa dạng của dữ liệu đến từ nhiều nguồn khác nhau
    như từ các thiết bị cảm biến, thiết bị di động, hay thông qua các trang mạng xã hội
  - **Veracity (tin cậy)**
    Bằng những phương tiện truyền thông xã hội bùng nổ như hiện nay
    và sự gia tăng mạnh mẽ về tương tác và chia sẻ của người dùng
    bức tranh toàn cảnh về độ chính xác cũng như tin cậy của thông tin
    ngày càng trở nên hỗn loạn và khó khăn
    Vấn đề phân tích và loại bỏ dữ liệu thiếu chính xác
    đang là một trong những vấn đề lớn của Big Data
  - **Value (giá trị)**
    Đây là đặc trưng **quan trọng nhất** bởi các thông tin đang tiềm ẩn
    trong bộ dữ liệu khổng lồ được trích xuất
    để phục vụ cho việc phân tích, dự báo
    Những thông tin này có thể được sử dụng trong rất nhiều lĩnh vực
    như kinh doanh, nghiên cứu khoa học, y học, vậy lý,...
    giúp giải quyết những vấn đề mà cuộc sống hiện đại đặt ra
- Videos:
  - [Big Data là gì?](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/24c4V/what-is-big-data)
  - [Tổng quan về Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/yqVpL/beyond-the-hype)
  - [Ảnh hưởng của Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/wLFkJ/impact-of-big-data)
  - [Nên sử dụng Big Data trong trường hợp nào?](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/570eD/big-data-use-cases)
- Ngoài ra, do dữ liệu của chúng ta sẽ được đến từ nhiều nguồn
  và đồng thời cũng sẽ ở nhiều dạng khác nhau
  Vậy nên chúng ta sẽ cần kết hợp các dữ liệu đó lại
  để đồng nhất với nhau và giúp dữ liệu giá trị hơn và liên kết hơn
  Công việc này sẽ bao gồm:
  - Làm giảm độ phức tạp của dữ liệu
  - Tăng mức độ phù hợp của dữ liệu
  - Chuyển đổi để các dữ liệu đồng nhất với nhau
  - Video: [Tích hợp dữ liệu ở các dạng khác nhau](https://www.coursera.org/learn/big-data-introduction/lecture/Ejk8J/the-key-integrating-diverse-data)

#### Ecosystems and tools for Big Data

- Chúng ta có hai cách để có thể xử lý các dữ liệu:
  - **Xử lý tuần tự (Linear processing):**
    - Các công việc xử lý dữ liệu sẽ được thực hiện tuần tự.
    - Các công việc trước cần phải hoàn thành thì mới có thể sang công việc tiếp theo
    - Nếu có lỗi xảy ra ở một bước đó thì các công việc sẽ phải
      dừng lại, hoặc chạy lại hết từ đầu sau khi xử lý các lỗi
  - **Xử lý song song (Parallel processing):**
    - Các công việc được xử lý song song, đọc lập với nhau
    - Nếu có 1 công việc bị lỗi thì cũng sẽ không ảnh hưởng
      đến các công việc còn lại và có thể dễ dàng thực hiện lại
  - ![linear & parallel processing](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_2..png?alt=media&token=0fdc5f0e-46e1-4adb-8637-717184fe9665)
- Có thể thấy rằng việc xử lý dữ liệu song song
  sẽ phù hợp hơn với Big Data nhờ vào một số ưu điểm sau:
  - Tiết kiệm thời gian xử lý dữ liệu
  - Tiết kiệm được các tài nguyên tính toán hơn cho các Node
  - Dễ dàng mở rộng hơn nếu dữ liệu nhiều hơn
- Với Big Data, để xử lý dữ liệu song song thì
  chúng ta sẽ chia hệ thống thành nhiều máy **(Cluster)** khác nhau
  mỗi máy có thể đảm nhiệm vai trò lưu trữ, tính toán hoặc cả hai
  Sau đó, chúng ta cần xử lý dữ liệu thì sẽ chạy song song với các Cluster khác nhau
  Kiến trúc này giúp cho dữ liệu có thể được xử lý song song và độc lập với nhau
- ![parallel processing clusters](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_3..png?alt=media&token=1fcea8af-5bd7-48c2-8db1-6b1da86e7fa6)
- Ngoài ra, kiến trúc này cũng giúp dễ dàng mở rộng theo chiều ngang
  tức là khi ta cần một tài nguyên lớn hơn để tính toán thì chỉ việc tạo thêm nhiều Cluster
  chứ không cần phải nâng cấp cấu hình cho các Cluster cũ
  Đồng thời cũng giúp tính bảo toàn tính toàn vẹn dữ liệu thông qua cơ chế **Fault tolerance**
  dữ liệu sẽ được tạo thành nhiều bản sao ở các Cluster khác nhau
  khi 1 Cluster gặp sự cố gây ra mất mát dữ liệu thì ta có thể dễ dàng backup lại
  Ta sẽ được tìm hiểu kỹ hơn ở bài về Hadoop HDFS
- [parallel processing clusters problem](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L1_4..png?alt=media&token=544bc364-9996-4696-ae90-fd241d154c2e)
- Video: [Xử lý dữ liệu song song và Scaling](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/ix1eD/parallel-processing-scaling-and-data-parallelism)

- Các công cụ trong hệ sinh thái Big Data sẽ được chia thành các công cụ như sau:
  - **Data Technologies**
    Sử dụng để xử lý, chia sẻ các dữ liệu ở định dạng khác nhau và giúp dữ liệu phân tán
  - **Analytics và Visualization**
    Giúp trực quan hóa và phân tích dữ liệu
  - **Business Intelligence**
    Giúp chuyển hóa các dữ liệu để dễ dàng truy cập, phục vụ cho các vấn đề nghiệp vụ
  - **Cloud Provider**
    Cung cấp các dịch vụ dưới dạng Cloud để dễ dàng mở rộng hơn
  - **NoSQL Database**
    Cơ sở dữ liệu NoSQL để lưu trữ dữ liệu
  - **Programming Tools**
    Các công cụ để xử lý các công việc đòi hỏi logic phức tạp trong vòng đời của Big Data
- Và đồng thời sẽ có những công cụ mã nguồn mở giúp chúng ta thao tác với Big Data dễ dàng hơn
- Video: [Hệ sinh thái và các công cụ về Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/WyhaU/big-data-tools-and-ecosystem)
- Video: [Các công cụ mã nguồn mở của Big Data](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/Fk5f4/open-source-and-big-data)

### Hadoop

#### Hadoop concepts

- Hadoop là một dạng Apache Framework
- Apache Hadoop là một mã nguồn mở cho phép sử dụng các distributed processing (ứng dụng phân tán) để quản lý và lưu trữ những tệp dữ liệu lớn
- Hadoop áp dụng mô hình MapReduce trong hoạt động xử lý Big Data
- Hadoop có những ưu điểm sau:
  - Cho phép ng dùng nhanh chóng viết và kiểm tra các hệ thống phân tán
    Đây là cách hiệu quả cho phép phân phối dữ liệu và công việc xuyên suốt các Cluster nhờ vào cơ chế xử lý song song của các lỗi CPU
  - Hệ thống Hadoop thiết kế sao cho các lỗi xảy ra trong hệ thống
    được xử lý tự động, không ảnh hưởng đến các ứng dụng phía trên
  - Có thể phát triển lên nhiều server với cấu trúc master-slave
    để đảm bảo thực các công việc linh hoạt và không bị ngắt quãng
    do chia nhỏ công việc cho các server slave được điều khiển bởi server master
  - Có thể tương thích trên mọi nền tảng như Window, Linux, MacOS do được tạo từ Java
- Tuy nhiên Hadoop vẫn có những khuyết điểm như:
  - thiết bị lưu trữ tốc độ chậm
  - máy tính thiếu tin cậy
  - lập trình song song phân tán không dễ dàng
- Video: [Giới thiệu về Hadoop](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/1sOjx/introduction-to-hadoop)
- Hadoop gồm 4 thành phần chính:
  - **Hadoop Common**
    Đây là các thư viện và tiện ích cần thiết của Java để các module khác sử dụng.
    Những thư viện này cung cấp hệ thống file và lớp OS trừu tượng,
    đồng thời chứa các mã lệnh Java để khởi động Hadoop.
  - **Hadoop YARN**
    Đây là framework để quản lý tiến trình và tài nguyên của các cluster.
  - **Hadoop Distributed File System (HDFS)**
    Đây là hệ thống file phân tán cung cấp truy cập thông lượng cao
    cho ứng dụng khai thác dữ liệu.
  - **Hadoop MapReduce**
    Đây là hệ thống dựa trên YARN dùng để xử lý song song các tập dữ liệu lớn.
- Hiện nay Hadoop đang ngày càng được mở rộng cũng như
  được nhiều framework khác hỗ trợ như Hive, Hbase, Pig.
  Tùy vào mục đích sử dụng mà ta sẽ áp dụng framework phù hợp
  để nâng cao hiệu quả xử lý dữ liệu của Hadoop.

- Video: [Hệ sinh thái Hadoop](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/g7y7n/hadoop-ecosystem)

#### HDFS

- Một trong những vấn đề lớn nhất của các hệ thống phân tích Big Data là quá tải
  Không phải hệ thống nào cũng đủ khỏe để có thể tiếp nhận một lượng thông tin khổng lồ như vậy
  Chính vì thế, nhiệm vụ của **Hadoop Distributed File System** là phân tán cung cấp truy cập thông lượng cao giúp cho ứng dụng chủ
  Cụ thể, khi HDFS nhận được một tệp tin, nó sẽ tự động chia file đó ra thành nhiều phần nhỏ
  Các mảnh nhỏ này được nhân lên nhiều lần và chia ra lưu trữ tại các máy chủ khác nhau
  để phân tán sức nặng mà dữ liệu tạo nên đồng thời cũng đảm bảo được tính toàn vẹn dữ liệu
- HDFS sử dụng các cấu trúc **master node** và **worker/slave node**
  Trong khi master node hay còn gọi là **Name Node** quản lý các file metadata
  thì **worker/slave node** chịu trách nhiệm lưu trữ dữ liệu
  Chính vì thế nên worker/slave node cũng được gọi là data node
  Một **Data node** sẽ chứa nhiều khối được phân nhỏ của tệp tin lớn ban đầu
  Dựa theo chỉ thị từ Master node, các Data node này sẽ trực tiếp điều hành hoạt động thêm, bớt những khối nhỏ của tệp tin
- ![HDFS architecture](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L2_1..png?alt=media&token=137107da-2f66-4c9d-9a2f-944266381b58)
- HDFS chỉ cung cấp 2 cơ chế truy cập dữ liệu là READ và WRITE
  - **READ**
    - Client sẽ gửi yêu cầu đến Name Node để biết được dữ liệu đang nằm ở Data Node nào.
      Sau đó sẽ trực tiếp đọc dữ liệu từ Data Node đó
  - **WRITE**
    - Name Node sẽ cần đảm bảo file đó chưa tồn tại,
      nếu đã tồn tại thì sẽ không báo lỗi.
    - Nếu như file chưa tồn tại vì client sẽ được cấp quyền để truy cập vào Data node để ghi dữ liệu
- Nguyên lý cốt lõi của HDFS như sau:
  - Chỉ ghi thêm (Append) giúp giảm chi phí điều khiển tương tranh
  - Phân tán dữ liệu
  - Nhân bản dữ liệu để bảo toàn tính toàn vẹn dữ liệu
  - Có cơ chế chịu lỗi:
    - Các Data Node có thể được backup nhờ sử dụng cơ chế nhân bản dữ liệu
    - Còn các Name Node cũng sẽ có Secondary Name Node để có thể thay thế nếu có lỗi
- Video: [HDFS](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/sfnQg/hdfs)

#### Map Reduce

- **Hadoop MapReduce** cho phép phân tán dữ liệu từ một máy chủ sang nhiều máy con
  Mỗi máy con này sẽ nhận một phần dữ liệu khác nhau và tiến hành xử lý cùng lúc
  Sau đó chúng sẽ báo lại kết quả lên máy chủ
  Máy chủ tổng hợp thông tin lại rồi trích xuất theo như yêu cầu của người dùng
- Các thực thi theo mô hình như vậy giúp tiết kiệm nhiều thời gian xử lý
  và cũng giảm gánh nặng lên hệ thống
- Chức năng của máy chủ là
  - quản lý tài nguyên
  - đưa ra thông báo
  - lịch trình hoạt động cho các Cluster
- Các cluster sẽ thực thi theo kế hoạch được định sẵn
  và gửi báo cáo dữ liệu lại cho máy chủ
  Tuy nhiên đây cũng là điểm yếu của hệ thống này
  Nếu máy chủ bị lỗi thì toàn bộ quá trình sẽ bị ngừng lại hoàn toàn
- Một chương trình Map Reduce có thể được lập trình với nhiều ngôn ngữ khác nhau
  ( Java, C+++, Python, R, Ruby)
  bạn sẽ cần phải cung cấp các hàm Map và Reduce để xác định các công việc phải làm
  hai hàm này được thực thi bởi các tiến trình Mapper và Reducer tương ứng.
- Trong chương trình MapReduce
  - dữ liệu được nhìn nhận như là các cặp khóa - giá trị (key-value)
  - các hàm Map và Reduce nhận đầu vào và trả về đầu ra các cặp (key-value)
- ![MapReduce flow](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L2_2..png?alt=media&token=bd81ffed-555e-4199-8dc7-db177d302175)
- Cơ chế hoạt động của MapReduce sẽ gồm các bước như sau
  - **Input** -> nạp dữ liệu cần xử lý
  - **Split** -> hệ thống sẽ chia dữ liệu thành các mảng cho từng máy
  - **Map** -> thực hiện song song hàm Map cho các mảnh dữ liệu đã chia trước đó
  - **Shuffle** -> sắp xếp và phân chia dữ liệu từ Map đưa vào các Reduce
  - **Reduce** -> thực hiện hàm Reduce cho các dữ liệu đã chia
  - **Output** -> gộp các dữ liệu đầu ra từ bước Reduce để trả về kết quả cuối cùng
- ![How MapReduce works](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L2_3.png?alt=media&token=91fe892c-b905-432e-b4e7-7b0ebdd38784)
- Video: [Map Reduce](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/nb5Lf/intro-to-mapreduce)

#### Others in Hadoop

##### YARN

- YARN : Yet Another Resource Negotiator
- YARN đóng vai trò cấp phát lượng tài nguyên phù hợp cho các ứng dụng khi có yêu cầu
- do Nodes có tài nguyên là
  bộ nhớ và CPU cores
  vậy nên các có cơ chế cấp phát hợp lý
- YARN cung cấp daemons và APIs cần thiết cho việc phát triển ứng dụng phân tán
  đồng thời xử lý và lập lịch sử dụng tài nguyên tính toán (CPU hay memory)
  cũng như giám sát quá trình thực thi các ứng dụng đó
- YARN tổng quát hơn MapReduce thể hệ đầu tiên
  (JobTracker/ TaskTracker)
- Video: [YARN](https://www.coursera.org/learn/big-data-introduction/lecture/Rn7sf/yarn-a-resource-manager-for-hadoop)

##### Hive

- Hive là một nền tảng được sử dụng để phát triển
  các tập lệnh gần giống SQL (HiveSQL)
  để thực hiện các hoạt động MapReduce
  từ đó giúp giảm thời gian phát triển mà vẫn có thể sử dụng được MapReduce
- Trình biên dịch Hive chạy trên các máy client
  giúp chuyển HiveQL script thành MapReduce jobs và đệ trình các công việc này lên cụm tính toán
- Video: [Hive](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/V8YLj/hive)

##### HBase

- HBase là một CSDL cột mở rộng phân tán, lưu trữ trên HDFS
- được xem như là hệ quản trị CSDL của Hadoop
- Dữ liệu được tổ chức về mặt logic là các bảng
  bao gồm rất nhiều dòng và cột
- Ưu điểm
  - Có tính khả mở cao, đáp ứng băng thông ghi dữ liệu tốc độ cao
  - Hỗ trợ hàng trăm ngàn thao tác INSERT mỗi giây (s)
- Nhược điểm

  - Do là NoSQL nên không có ngôn ngữ truy vấn mức cao như SQL và phải sử dụng API để scan/put/get dữ liệu theo khóa

- Video: [HBase](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/pI5Tx/hbase)

### Monitor and process Big Data

#### Big Data system management

##### Data Ingestion - bổ sung dữ liệu

- Quy trình bổ sung dữ liệu cho hệ thống nắm vai trò rất quan trọng
  và cũng tiêu tốn khá nhiều tài nguyên
- Các câu hỏi giúp xác định được quy trình bổ sung dữ liệu phù hợp:
  - Có bao nhiêu Data Source?
  - Độ lớn của dữ liệu?
  - Số Data Source có tăng lên nữa không?
  - Thời gian sẽ cần bổ sung thêm dữ liệu?
  - Bạn sẽ làm gì với các dữ liệu không tốt?
  - Bạn sẽ làm gì nếu lượng dữ liệu quá ít hoặc quá nhiều?
- Video: [Data Ingestion](https://www.coursera.org/learn/big-data-management/lecture/Lnlwf/data-ingestion)

##### Data storage - lưu trữ dữ liệu

- Tương tự với việc bổ sung dữ liệu thì lưu trữ dữ liệu cũng có vai trò quan trọng
- Sẽ có 2 vấn đề ở phần này
  - Cần phân bổ lưu trữ dữ liệu như thế nào
    Bao nhiêu dữ liệu cần lưu trữ và sẽ lưu trữ theo cách nào
    (truy cập trực tiếp hay truy cập qua Internet)
  - Yêu cầu về tốc độ đọc/ ghi dữ liệu như thế nào
- Sau khi có đáp án cho các câu hỏi trên
  thì ta có thể chọn được 1 cách lưu trữ dữ liệu để đáp ứng các yêu cầu đó
- Video: [Data Storage](https://www.coursera.org/learn/big-data-management/lecture/RplBY/data-storage)

##### Data quality - Chất lượng dữ liệu

- có 3 lý do chính mà bạn cần đảm bảo dữ liệu của bạn chất lượng
  - chúng ta sẽ trích xuất thông tin hữu ích từ dữ liệu
    vậy nên nếu ban đầu dữ liệu đã không chất lượng
    thì có thể làm ảnh hưởng đến các phân tích nghiệp vụ sau này
  - Dữ liệu liên quan đến các ngành như y tế, ngân hàng, ...
    nếu có sai sót thì có thể liên quan đến cả các vấn đề pháp lý
  - Nếu dữ liệu chúng ta cung cấp cho khách hàng
    hoặc các bên liên quan không tốt thì có thể làm giảm uy tín
- Video: [Data quality](https://www.coursera.org/learn/big-data-management/lecture/xdrBU/data-quality)

##### Data operations - Các thao tác làm việc với dữ liệu

- Để phù hợp với một yêu cầu nào đó,
  ta cần sử dụng các thao tác để làm việc với dữ liệu
- Các thao tác có thể được chia làm thành 2 loại
  - Thao tác trên 1 data item
  - Thao tác trên 1 tập hợp các dữ liệu
    Thao tác này còn có thể được chia nhỏ hơn thành các loại như sau:
    - Thao tác để chọn một phần dữ liệu (lấy subset)
    - Thao tác để kết hợp các tập hợp dữ liệu (merge)
    - Thao tác tính toán trên tập dữ liệu (đếm dữ liệu,..)
- Video: [Data operations](https://www.coursera.org/learn/big-data-management/lecture/3iZzB/data-operations)

##### Khả năng mở rộng và bảo mật dữ liệu

- Chúng ta có hai cách để mở rộng cho môi trường Big Data
  - **Mở rộng theo chiều dọc**
    - Nâng cấp cấu hình phần cứng cho các máy trên hệ thống
    - Cách này giúp các thao tác được thực hiện tốt hơn
      nhưng đồng thời cũng rất khó triển khai và maintain
  - **Mở rộng theo chiều ngang**
    - Bổ sung thêm các máy mới vào hệ thống
    - Cách này thường được dùng với các hệ thống xử lý phân tán
      tuy có thể hiệu suất sẽ kém hơn chút nhưng lại dễ dàng để triển khai và maintain
      -> Các môi trường Big Data thường sử dụng mở rộng theo chiều ngang
- Ngoài ra, bảo mật cũng là 1 công việc quan trọng
  Một số lưu ý về vấn đề bảo mật
  - Tăng số lượng máy trong hệ thống
    đồng nghĩa với việc quản lý bảo mật khó khăn hơn
  - Các dữ liệu được vận chuyển qua lại giữa các máy
    cũng cần phải đảm bảo tính bảo mật
  - Việc mã hóa và giải mã sẽ tốn nhiều tài nguyên
    nên ta cần phải sử dụng hợp lý
- Video: [Data Scalability & Security](https://www.coursera.org/learn/big-data-management/lecture/JEcdQ/data-scalability-and-security)

##### Tài liệu tham khảo

- Bạn có thể tham khảo thêm các tài liệu dưới đây về việc quản lý Big Data
- Tham khảo: [Energy Data Management Challenges at ConEd](https://www.coursera.org/learn/big-data-management/lecture/r7dn2/energy-data-management-challenges-at-coned)
- Tham khảo: [Gaming Industry Data Management: Q&A with Apmetrix CTO Mark Caldwell](https://www.coursera.org/learn/big-data-management/lecture/uDoPs/gaming-industry-data-management-q-a-with-apmetrix-cto-mark-caldwell)
- Tham khảo: [Flight Data Management at FlightStats: A Lecture by CTO Chad Berkley](https://www.coursera.org/learn/big-data-management/lecture/ITqCN/flight-data-management-at-flightstats-a-lecture-by-cto-chad-berkley)

#### Big Data processing

##### Big Data Pipeline

- Hầu hết các ứng dụng BigData được cấu tạo bởi nhiều thao tác kết hợp lại với nhau
  ta gọi đó là Pipeline
- MapReduce cũng có thể được gọi là 1 **Big Data Pipelines** do gồm các thao tác như Input, Split, Map, Shuffle, Reduce và Output
- Video: [Big Data Processing Pipelines](https://www.coursera.org/learn/big-data-integration-processing/lecture/c4Wyd/big-data-processing-pipelines)

##### Một số thao tác thường dùng trong Big Data Pipelines

- **Map**
  Áp dụng một thao tác nào đó trên toàn bộ phần tử của dữ liệu
- **Reduce**
  Kết hợp các dữ liệu theo các giá trị **key**, đầu vào của thao tác này thường là các dữ liệu có dạng **key-value**
- **Cross/Catesian**
  Áp dụng một thao tác nào đó trên từng cặp dữ liệu từ hai Sets
- **Match/Join**
  Đầu vào của thao tác này thường là các tập dữ liệu có dạng **key-value**
  sau đó áp dụng thao tác nào đó lên những cặp dữ liệu cùng giá trị **key** từ 2 tập đầu vào
- **Co-Group**
  Nhóm các phần tử với nhau
- **Filter**
  Lọc các phần tử thỏa mãn điều kiện
- Video: [Một số thao tác trong Big Data Processing Pipelines](https://www.coursera.org/learn/big-data-integration-processing/lecture/WZPCd/some-high-level-processing-operations-in-big-data-pipelines)

##### Aggregation trong Big Data Pipelines

- **GROUP BY** - chia nhóm các dữ liệu dựa trên một điều kiện nào đó
- Các thao tác tính toán như: **AVERAGE, MIN, MAX, STANDARD DEVIATION**
- Đồng thời, bạn cũng có thể kết hợp tuần tự các Aggregation với nhau cho các thao tác phức tạp hơn
- Video: [Các toán tử Aggregation trong Big Data Processing Pipelines](https://www.coursera.org/learn/big-data-integration-processing/lecture/VDBgw/aggregation-operations-in-big-data-pipelines)

##### Tổng quan Big Data processing

- 1 Hệ thống Big Data Processing sẽ có các tầng như sau:
  - **Data Management and Store**
    Tầng này sẽ chứa các thành phần giúp quản lý và lưu trữ các dữ liệu như HDFS
  - **Data Integration and Processing**
    Tầng này chứa các thành phần để dữ liệu được truy xuất, tích hợp và phân tích
    như YARN, Map Reduce,....
  - **Coordination and Workflow management**
    Tầng này chứa các thành phần để quản lý
    điều phối công việc cho các công cụ của 2 hai tầng còn lại
- Video: [Tổng quan về các hệ thống Big Data Processing](https://www.coursera.org/learn/big-data-integration-processing/lecture/Z1jrr/overview-of-big-data-processing-systems)

## Big Data with Spark

### Spark

- **Apache Spark**
  - là 1 framework mã nguồn mở
  - cho phép bạn xây dựng nhưng mô hình dự đoán nhanh chóng
    với khả năng thực hiện tính toán cùng lúc trên một nhóm các máy tính
    hay trên toàn bộ các tập dữ liệu
    mà không cần thiết phải trích xuất các mẫu tính toán thử nghiệm
  - tốc độ xử lý dữ liệu có được là do khả năng được thực hiện các tính toán
    trên nhiều máy tính khác nhau trên cùng 1 lúc tại bộ nhớ (in-memories)
    hay hoàn toàn trên RAM
- **Spark** được ưa chuộng hơn **Hadoop** trên một số ứng dụng nhờ tốc độ của mình
  (chính xác hơn là cấu trúc tính toán MapReduce trên Hadoop)
- Ưu điểm:
  thay vì lưu dữ liệu và các kết quả trung gian trong quá trình tính toán
  dưới dạng của các file hệ thống như Hadoop Map-Reduce nó lưu
  trong bộ nhớ của các Node
- Nhược điểm:
  không có hệ thống tập tin phân tán vì người ta thường chạy nó trên nền của Hadoop
- Spark cũng rất dễ dàng sử dụng từ Python, Java hoặc Scala
- Spark sẽ gồm các thành phần chính như sau

  - **Spark Core**
    - Được xem là nền tảng và điều kiện cho sự vận hành của các thành phần còn lại
    - Thành phần này đảm nhận vai trò thực hiện các công việc tính toán, xử lý trong bộ nhớ
      và tham chiếu các dữ liệu được lưu trữ bên ngoài
  - **Spark SQL**
    - Thành phần này giúp thực hiện các thao tác trên các Dataframe
      bằng các ngôn ngữ như Java, Scala hay Python thông qua sự hỗ trợ của ngôn ngữ SQL
    - Cung cấp SchemaRDD với mục đích hỗ trợ cho các kiểu **structured data** và **semi-structured data**.
  - **Spark Streaming**
    - Mục đích sử dụng cho thành phần này chính là coi stream là các mini batches
      và thực hiện các kỹ thuật RDD transformation với các dữ liệu này để phân tích stream
      Điều này giúp việc xử lý stream và phát triển lambda architecture
      trở nên dễ dàng bằng cách tận dụng lại các đoạn code được viết để xử lý batch
  - **MLlib**
    - Là một nền tảng học máy
    - Spark MLlib nhanh hơn gấp 9 lần so với phiên bản chạy trên Hadoop
      (theo so sánh từ benchmark) nhờ kiến trúc phân tán dựa trên bộ nhớ
  - **GraphX**
    - Đây là nền tảng xử lý các đồ thị dựa trên Spark
    - Nó cung cấp các API và được sử dụng để diễn tả tất cả các tính toán có trong đồ thị

- Video: [Khái niệm về Apache Spark](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3708614#questions)

- Tham khảo: [Spark 3 có gì mới?](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/17919510#questions)

### Spark Architecture

- Kiến trúc của Spark bao gồm
  - **Driver (trình điều khiển)**
  - **Executor Processes (trình thực thi)**
- _Driver_ tạo ra **_Spark Job_** và **_Spark Context_**
  chia các Job thành các Task có thể chạy song song trong các trình thực thi trên Cluster
  mỗi ứng dụng sẽ có 1 _Spark Context_ điều phối Executor
- **Spark Job**
  là các công việc tính toán mà ứng dụng cần thực hiện
  và có thể được thực thi song song
- **Spark Task**
  thực hiện các Spark Job trên các phân vùng dữ liệu (Partitions) khác nhau
  và cũng có thể được thực thi song song
- ![Apache Spark Architecture](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L4_1.png?alt=media&token=9dce6761-35ca-447c-976f-25454eb47a36)
- **Stages**
  - là một tập hợp các Task được phân tách bằng một lần _Shuffles_ dữ liệu
    - việc Shuffles rất tốn kém
      vì chúng yêu cầu tuần tự hóa dữ liệu, đĩa và mạng I/O
    - Shuffles sẽ cần thiết
      nếu như các thao tác cần dữ liệu từ các phân vùng (Partitions) khác
      ví dụ: ![Shuffle example](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L4_2.png?alt=media&token=734cfe98-f5a4-421a-af8d-c1bd3519476f)
- Driver (chương trình điều khiển) có thể được chạy ở 2 chế độ sau
  - **Client Mode**
    Trình gửi ứng dụng (Application Submitter)
    chẳng hạn như thiết bị đầu cuối của máy người dùng khởi chạy trình điều khiển bên ngoài Cluster
  - **Cluster Mode**
    Chương trình điều khiển được gửi đến và chạy trên một Worker Node có sẵn bên trong Cluster
  - ![driver running modes](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Engineer%2FDEP303x%2FSumary_Image%2FDEP303_sum_L4_3.png?alt=media&token=613e7952-b1b7-4563-b287-e4f3913f3d7e)
- Video: [Kiến trúc của Apache Spark](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/lecture/7D9xA/apache-spark-architecture)
- Tham khảo: [Sự khác biệt giữa SparkSession và SparkContext](https://sparkbyexamples.com/spark/sparksession-vs-sparkcontext/)

#### Cách cài đặt và chạy chương trình Spark

- Cách 1: Cài đặt trực tiếp trên máy tính
  - Video: [Cài đặt môi trường để sử dụng Spark](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/5586888#questions)
  - Tham khảo: [Cài đặt Pyspark qua Pip hoặc Conda](https://spark.apache.org/docs/3.1.1/api/python/getting_started/install.html)
- Cách 2: Sử dụng trên Google Colab
  - Google Colab là một dạng Jupyter Notebook tùy biến cho phép thực thi Python
    trên nền tảng đám mây, được cung cấp Google
  - Sử dụng Google Colab có những lợi ích ưu việt như:
    - sẵn sàng chạy Python ở bất kỳ thiết bị nào có kết nối internet mà ko cần cài đặt
    - chia sẻ và làm việc nhóm dễ dàng, sử dụng miễn phí GPU cho các dự án về AI
- Trong bài Lab này ta dùng Google collab
  - Tham khảo: [Hướng dẫn cài đặt và làm quen Google Colab cơ bản](https://trituenhantao.io/lap-trinh/lam-quen-voi-google-colab/)
  - Tham khảo: [Hướng dẫn sử dụng Google Colab chi tiết](https://thinhvu.com/2021/07/29/huong-dan-su-dung-google-colab-tutorial-101/)
  - Tham khảo: [Cài đặt và sử dụng PySpark trên Google Colab](https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/)
- Sau khi đã cài đặt thành công Spark, ta sẽ có 2 cách để chạy được một Spark Job
  - **Spark Shell**
    - là một trình tương tác giúp ta viết và chạy trực tiếp các câu lệnh PySpark
    - là một công cụ khá hữu ích để làm quen với các Spark API cũng như thực hiện các thao tác phân tích dữ liệu
  - **spark-submit**
    - như tên gọi thì bạn sẽ submit một Spark Job và thực hiện Job đó
    - công việc của các Job này sẽ được khai báo bằng các đoạn code Python
- Hoặc nếu ta sử dụng một thư viện là pyspark
  ta có thể chạy 1 SparkJob bằng cách gọi `python <file-name>.py`
  - Video: [Thực hành: Cài đặt Dataset](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3702584#questions)
  - Video: [Thực hành: Chạy ứng dụng Spark đầu tiên](https://funix.udemy.com/course/taming-big-data-with-apache-spark-hands-on/learn/lecture/3700612#questions)

### Lab 1 - Setup and run Spark

- [Lab1: Cài đặt và sử dụng Spark](https://colab.research.google.com/drive/1JKmBBvT7hv0civmZU__G923H5wKAiJN5?usp=sharing)
- [Hướng dẫn](https://docs.google.com/document/d/19aXDWH86NaVJ-sAWm9YERHN4KT2Pqe70g162_NITXwc/edit?usp=sharing)
- [Bài làm](https://colab.research.google.com/drive/1BRAIycbiRyKvSIf1eiiaM2GiJqxpjTlk?authuser=1#scrollTo=e_fAhSKr9uhE)

### Spark RDD

#### Resilient Distributed Datasets

- RDD là một cấu trúc cơ bản của Spark

#### Transformations and Actions trong RDD

## Apache Airflow
